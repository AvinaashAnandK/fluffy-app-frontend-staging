[{"role": "system", "content": "###INSTRUCTION###\nYou are Fluffy, a tech solutions expert and maintainer of the repo https://github.com/huggingface/transformers. Your task is to help developers in addressing a user query related to the opensource github repo.\nUse the context provided below to answer the user query.\n    Guidelines for Fluffy:\n    1. Think sequentially, breaking the user query step by step.\n    2. When uncertain, answer with \"I don't know\".\n    3. When there is any question asked outside the scope of the repository, respond with \u201cI can only assist you in understanding how the repo works better.\u201d\n"}, {"role": "user", "content": "past_key_values for SeamlessM4Tv2ForSpeechToText is not working as expected\nI have segments of audio, and I would like to pass past_key_values between them. I exptected the transcribtion quality to increase, but rather it became unreadable.\n\nprocessor = AutoProcessor.from_pretrained(path_to_model)\nmodel = SeamlessM4Tv2ForSpeechToText.from_pretrained(path_to_model)\naudio_chunks = [audio_segments]\n\npast_key_values= None\n\nfor i in range(5):\n    audio_inputs = processor(audios=audio_chunks[i], return_tensors=\"pt\", sampling_rate=16_000)\n\n    output = model.generate(**audio_inputs, tgt_lang=\"rus\", repetition_penalty=1.1, return_dict_in_generate=True, past_key_values=past_key_values)\n    \n    tmp_result = processor.decode(output[0][0], skip_special_tokens=True)\n    past_key_values = output['past_key_values']\n\nExpected behavior\nThe transcription quality is supposed to increase when I pass past_key_values (or at least have similar transcribation quality as when past_key_values=None).\nThe audio is the same. In other words, I had some audio, applied VAD to segment it into batches, and then fed this segments to the model one by one.\n"}, {"role": "user", "content": "BELOW ARE RELEVANT CODE CHUNKS FOR YOUR REFERENCE\n RELEVANT CODE FILES IN REPO:\n CODE FILE PATH#1: src/transformers/models/seamless_m4t/modeling_seamless_m4t.py\n\n CODE EXPLANATION: The provided code defines the SeamlessM4T model, which is a multimodal transformer capable of handling various tasks such as speech-to-text (S2TT), text-to-text (T2TT), text-to-speech (T2ST), and speech-to-speech (S2ST) translation and synthesis. The model is part of the Hugging Face Transformers library and is designed to work with different modalities, including text and speech.\n\nHere are the key components and concepts in the code:\n\n1. **Shared Embeddings**: The model uses shared embeddings (`self.shared`) for the encoder and decoder, which means the same embedding layer is used to convert input token IDs to vectors for both encoding and decoding.\n\n2. **Encoders and Decoders**: The model has separate encoders for text (`self.text_encoder`) and speech (`self.speech_encoder`), and a shared decoder (`self.text_decoder`). The encoders process the input data and generate hidden states, which are then used by the decoder to generate the output.\n\n3. **Language Model Head**: The `lm_head` is a linear layer that projects the decoder's output to the vocabulary size, generating the logits for the next token prediction.\n\n4. **Sub-Models**: The main model (`SeamlessM4TModel`) uses sub-models for specific tasks:\n   - `SeamlessM4TTextToUnitForConditionalGeneration`: A text-to-unit model that generates a sequence of discrete units (tokens) from text input.\n   - `SeamlessM4TCodeHifiGan`: A vocoder model that converts the generated units into an audio waveform.\n\n5. **Generation**: The `generate` method is used to produce outputs from the model. It can generate text or speech based on the input modality and the specified target language. The method handles different generation strategies and can be customized with various parameters.\n\n6. **Modality Handling**: The model can switch between text and speech modalities using the `set_modality` method. This changes the main input name (`self.main_input_name`) and affects how the model processes inputs.\n\n7. **Weight Tying**: The model ties weights between the shared embedding layer and the language model head to reduce the number of parameters and potentially improve performance.\n\n8. **Forward Pass**: The `forward` method defines how the model processes inputs and generates outputs. It handles different input types (IDs or features) and passes data through the appropriate encoder and decoder.\n\n9. **Utilities**: The code includes various utility functions and classes for handling attention masks, positional embeddings, and other components necessary for the model's operation.\n\n10. **Configuration**: The model uses a configuration object (`config`) to set various parameters, such as the number of layers, hidden sizes, vocabulary sizes, and other model-specific settings.\n\nIn summary, the SeamlessM4T model is a complex and versatile architecture that can handle multiple modalities and perform a variety of tasks. It leverages shared components, sub-models, and a flexible generation mechanism to achieve its goals.\n\n CODE:\n```\n# coding=utf-8\n# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch SeamlessM4T model.\"\"\"\n\n\nimport copy\nimport math\nfrom dataclasses import dataclass\nfrom typing import Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import Tensor, nn\nfrom torch.nn import CrossEntropyLoss\n\nfrom ...activations import ACT2FN\nfrom ...deepspeed import is_deepspeed_zero3_enabled\nfrom ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\nfrom ...modeling_outputs import (\n    BaseModelOutput,\n    BaseModelOutputWithPastAndCrossAttentions,\n    Seq2SeqLMOutput,\n    Seq2SeqModelOutput,\n    Wav2Vec2BaseModelOutput,\n)\nfrom ...modeling_utils import PreTrainedModel\nfrom ...utils import (\n    ModelOutput,\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n)\nfrom .configuration_seamless_m4t import SeamlessM4TConfig\n\n\nlogger = logging.get_logger(__name__)\n\n_CHECKPOINT_FOR_DOC = \"facebook/hf-seamless-m4t-medium\"\n_CONFIG_FOR_DOC = \"SeamlessM4TConfig\"\n\nSEAMLESS_M4T_PRETRAINED_MODEL_ARCHIVE_LIST = [\n    \"facebook/hf-seamless-m4t-medium\",\n    # See all SeamlessM4T models at https://huggingface.co/models?filter=seamless_m4t\n]\n\nSPEECHT5_PRETRAINED_HIFIGAN_CONFIG_ARCHIVE_MAP = {\n    \"microsoft/speecht5_hifigan\": \"https://huggingface.co/microsoft/speecht5_hifigan/resolve/main/config.json\",\n}\n\n\n@dataclass\nclass SeamlessM4TGenerationOutput(ModelOutput):\n    \"\"\"\n    Class defining the generated outputs from [`SeamlessM4TModel`], [`SeamlessM4TForTextToText`],\n    [`SeamlessM4TForTextToSpeech`], [`SeamlessM4TForSpeechToSpeech`] and [`SeamlessM4TForTextToSpeech`].\n\n    Args:\n        waveform (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n            The final audio waveform predicted by the model.\n        waveform_lengths (`torch.IntTensor` of shape `(batch_size,)`, *optional*):\n            The length in samples of each element in the `waveform` batch.\n        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            The generated translated sequences. This is the output of the text-to-text or the speech-to-text models.\n            The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished\n            early due to the `eos_token_id`.\n        unit_sequences (`torch.LongTensor` of shape `(batch_size, unit_sequence_length)`, *optional*):\n            The generated translated unit sequences. This is the output of the text-to-units model. The second\n            dimension (unit_sequence_length) is either equal to `t2u_max_length` or shorter if all batches finished\n            early due to the `t2u_eos_token_id`.\n    \"\"\"\n\n    waveform: Optional[torch.FloatTensor] = None\n    waveform_lengths: Optional[torch.IntTensor] = None\n    sequences: Optional[Tuple[torch.FloatTensor]] = None\n    unit_sequences: Optional[Tuple[torch.FloatTensor]] = None\n\n\nSEAMLESS_M4T_START_DOCSTRING = r\"\"\"\n    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) sub-class. Use\n    it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n    behavior.\n\n    Parameters:\n        config ([`~SeamlessM4TConfig`]): Model configuration class with all the parameters of the model.\n            Initializing with a config file does not load the weights associated with the model, only the\n            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\nSEAMLESS_M4T_INPUTS_DOCSTRING_FIRST_PART = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See\n            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n            [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n    \"\"\"\n\nSEAMLESS_M4T_INPUTS_DOCSTRING_TEXT_PART = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Indices of input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See\n            [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        \"\"\"\n\nSEAMLESS_M4T_INPUTS_DOCSTRING_SPEECH_PART = r\"\"\"\n    Args:\n        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n            [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n        \"\"\"\n\nSEAMLESS_M4T_INPUTS_DOCSTRING_LAST_PART = r\"\"\"\n        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Indices of decoder input sequence tokens in the vocabulary.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are decoder input IDs?](../glossary#decoder-input-ids)\n\n            Bart uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n\n            For translation and summarization training, `decoder_input_ids` should be provided. If no\n            `decoder_input_ids` is provided, the model will create this tensor by shifting the `input_ids` to the right\n            for denoising pre-training following the paper.\n        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n            be used by default.\n\n            If you want to change padding behavior, you should read [`modeling_bart._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape`(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, target_sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `decoder_input_ids` you can choose to directly pass an embedded\n            representation. If `past_key_values` is used, optionally only the last `decoder_inputs_embeds` have to be\n            input (see `past_key_values`). This is useful if you want more control over how to convert\n            `decoder_input_ids` indices into associated vectors than the model's internal embedding lookup matrix.\n\n            If `decoder_input_ids` and `decoder_inputs_embeds` are both unset, `decoder_inputs_embeds` takes the value\n            of `inputs_embeds`.\n        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\nM4T_MODEL_INPUTS_DOCSTRING = SEAMLESS_M4T_INPUTS_DOCSTRING_FIRST_PART + SEAMLESS_M4T_INPUTS_DOCSTRING_LAST_PART\n\nM4T_TEXT_INPUTS_DOCSTRING = SEAMLESS_M4T_INPUTS_DOCSTRING_TEXT_PART + SEAMLESS_M4T_INPUTS_DOCSTRING_LAST_PART\n\nM4T_SPEECH_INPUTS_DOCSTRING = SEAMLESS_M4T_INPUTS_DOCSTRING_SPEECH_PART + SEAMLESS_M4T_INPUTS_DOCSTRING_LAST_PART\n\n\n############ UTILS ################\n\n\n# Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\ndef create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n    \"\"\"\n    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n    are ignored. This is modified from fairseq's `utils.make_positions`.\n\n    Args:\n        x: torch.Tensor x:\n\n    Returns: torch.Tensor\n    \"\"\"\n    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n    mask = input_ids.ne(padding_idx).int()\n    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n    return incremental_indices.long() + padding_idx\n\n\n# Copied from transformers.models.bart.modeling_bart.shift_tokens_right\ndef shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n    shifted_input_ids[:, 0] = decoder_start_token_id\n\n    if pad_token_id is None:\n        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n    # replace possible -100 values in labels by `pad_token_id`\n    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n\n    return shifted_input_ids\n\n\ndef _compute_new_attention_mask(hidden_states: torch.Tensor, seq_lens: torch.Tensor):\n    \"\"\"\n    Computes an attention mask of the form `(batch, seq_len)` with an attention for each element in the batch that\n    stops at the corresponding element in `seq_lens`.\n\n    Args:\n        hidden_states (`torch.FloatTensor` of shape `(batch, seq_len, *)`):\n            The sequences to mask, where `*` is any number of sequence-specific dimensions including none.\n        seq_lens (`torch.Tensor` of shape `(batch)`:\n            Each element represents the length of the sequence at the same index in `hidden_states`\n\n    Returns:\n        `torch.FloatTensor`: The float attention mask of shape `(batch, seq_len)`\n    \"\"\"\n    batch_size, mask_seq_len = hidden_states.shape[:2]\n\n    indices = torch.arange(mask_seq_len, device=seq_lens.device).expand(batch_size, -1)\n\n    bool_mask = indices >= seq_lens.unsqueeze(1).expand(-1, mask_seq_len)\n\n    mask = hidden_states.new_ones((batch_size, mask_seq_len))\n\n    mask = mask.masked_fill(bool_mask, 0)\n\n    return mask\n\n\ndef format_speech_generation_kwargs(kwargs):\n    \"\"\"\n    Format kwargs for SeamlessM4T models that generate speech, attribute kwargs to either the text generation or the\n    speech generation models.\n\n    Args:\n        kwargs (`dict`)`:\n             Keyword arguments are of two types:\n\n                - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,\n                except for `decoder_input_ids` which will only be passed through the text components.\n                - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the\n                text model and speech model respectively. It has the priority over the keywords without a prefix.\n\n                This means you can, for example, specify a generation strategy for one generation but not for the\n                other.\n    \"\"\"\n    # attribute kwargs to models\n    kwargs_text = {}\n    kwargs_speech = {}\n    for key, value in kwargs.items():\n        if key.startswith(\"text_\"):\n            key = key[len(\"text_\") :]\n            kwargs_text[key] = value\n        elif key.startswith(\"speech_\"):\n            key = key[len(\"speech_\") :]\n            kwargs_speech[key] = value\n        else:\n            # If the key is already in a specific config, then it's been set with a\n            # submodules specific value and we don't override\n            if key not in kwargs_text:\n                kwargs_text[key] = value\n            if key not in kwargs_speech:\n                kwargs_speech[key] = value\n    return kwargs_text, kwargs_speech\n\n\n############ SPEECH ENCODER related code ################\n\n\n# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->SeamlessM4TConformer, feat_extract_activation->speech_encoder_hidden_act\nclass SeamlessM4TConformerPositionalConvEmbedding(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.conv = nn.Conv1d(\n            config.hidden_size,\n            config.hidden_size,\n            kernel_size=config.num_conv_pos_embeddings,\n            padding=config.num_conv_pos_embeddings // 2,\n            groups=config.num_conv_pos_embedding_groups,\n        )\n\n        weight_norm = nn.utils.weight_norm\n        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n            weight_norm = nn.utils.parametrizations.weight_norm\n\n        if is_deepspeed_zero3_enabled():\n            import deepspeed\n\n            with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n            deepspeed.zero.register_external_parameter(self, self.conv.weight_v)\n            deepspeed.zero.register_external_parameter(self, self.conv.weight_g)\n        else:\n            self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n\n        self.padding = SeamlessM4TConformerSamePadLayer(config.num_conv_pos_embeddings)\n        self.activation = ACT2FN[config.speech_encoder_hidden_act]\n\n    def forward(self, hidden_states):\n        hidden_states = hidden_states.transpose(1, 2)\n\n        hidden_states = self.conv(hidden_states)\n        hidden_states = self.padding(hidden_states)\n        hidden_states = self.activation(hidden_states)\n\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\n# Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerRotaryPositionalEmbedding with Wav2Vec2->SeamlessM4T, num_attention_heads->speech_encoder_attention_heads\nclass SeamlessM4TConformerRotaryPositionalEmbedding(nn.Module):\n    \"\"\"Rotary positional embedding\n    Reference : https://blog.eleuther.ai/rotary-embeddings/ Paper: https://arxiv.org/pdf/2104.09864.pdf\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        dim = config.hidden_size // config.speech_encoder_attention_heads\n        base = config.rotary_embedding_base\n\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n        self.cached_sequence_length = None\n        self.cached_rotary_positional_embedding = None\n\n    def forward(self, hidden_states):\n        sequence_length = hidden_states.shape[1]\n\n        if sequence_length == self.cached_sequence_length and self.cached_rotary_positional_embedding is not None:\n            return self.cached_rotary_positional_embedding\n\n        self.cached_sequence_length = sequence_length\n        # Embeddings are computed in the dtype of the inv_freq constant\n        time_stamps = torch.arange(sequence_length).type_as(self.inv_freq)\n        freqs = torch.einsum(\"i,j->ij\", time_stamps, self.inv_freq)\n        embeddings = torch.cat((freqs, freqs), dim=-1)\n\n        cos_embeddings = embeddings.cos()[:, None, None, :]\n        sin_embeddings = embeddings.sin()[:, None, None, :]\n        # Computed embeddings are cast to the dtype of the hidden state inputs\n        self.cached_rotary_positional_embedding = torch.stack([cos_embeddings, sin_embeddings]).type_as(hidden_states)\n        return self.cached_rotary_positional_embedding\n\n\n# Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerRelPositionalEmbedding with Wav2Vec2->SeamlessM4T\nclass SeamlessM4TConformerRelPositionalEmbedding(nn.Module):\n    \"\"\"Relative positional encoding module.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.max_len = config.max_source_positions\n        self.d_model = config.hidden_size\n        self.pe = None\n        self.extend_pe(torch.tensor(0.0).expand(1, self.max_len))\n\n    def extend_pe(self, x):\n        # Reset the positional encodings\n        if self.pe is not None:\n            # self.pe contains both positive and negative parts\n            # the length of self.pe is 2 * input_len - 1\n            if self.pe.size(1) >= x.size(1) * 2 - 1:\n                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n                return\n        # Suppose `i` is the position of query vector and `j` is the\n        # position of key vector. We use positive relative positions when keys\n        # are to the left (i>j) and negative relative positions otherwise (i<j).\n        pe_positive = torch.zeros(x.size(1), self.d_model)\n        pe_negative = torch.zeros(x.size(1), self.d_model)\n        position = torch.arange(0, x.size(1), dtype=torch.int64).float().unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, self.d_model, 2, dtype=torch.int64).float() * -(math.log(10000.0) / self.d_model)\n        )\n        pe_positive[:, 0::2] = torch.sin(position * div_term)\n        pe_positive[:, 1::2] = torch.cos(position * div_term)\n        pe_negative[:, 0::2] = torch.sin(-1 * position * div_term)\n        pe_negative[:, 1::2] = torch.cos(-1 * position * div_term)\n\n        # Reverse the order of positive indices and concat both positive and\n        # negative indices. This is used to support the shifting trick\n        # as in https://arxiv.org/abs/1901.02860\n        pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n        pe_negative = pe_negative[1:].unsqueeze(0)\n        pe = torch.cat([pe_positive, pe_negative], dim=1)\n        self.pe = pe.to(device=x.device, dtype=x.dtype)\n\n    def forward(self, hidden_states: torch.Tensor):\n        self.extend_pe(hidden_states)\n        start_idx = self.pe.size(1) // 2 - hidden_states.size(1) + 1\n        end_idx = self.pe.size(1) // 2 + hidden_states.size(1)\n        relative_position_embeddings = self.pe[:, start_idx:end_idx]\n\n        return relative_position_embeddings\n\n\n# Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSamePadLayer with Wav2Vec2->SeamlessM4T\nclass SeamlessM4TConformerSamePadLayer(nn.Module):\n    def __init__(self, num_conv_pos_embeddings):\n        super().__init__()\n        self.num_pad_remove = 1 if num_conv_pos_embeddings % 2 == 0 else 0\n\n    def forward(self, hidden_states):\n        if self.num_pad_remove > 0:\n            hidden_states = hidden_states[:, :, : -self.num_pad_remove]\n        return hidden_states\n\n\nclass SeamlessM4TConformerFeatureProjection(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.layer_norm = nn.LayerNorm(config.feature_projection_input_dim, eps=config.layer_norm_eps)\n        self.projection = nn.Linear(config.feature_projection_input_dim, config.hidden_size)\n        self.dropout = nn.Dropout(config.speech_encoder_dropout)\n\n    def forward(self, hidden_states):\n        # non-projected hidden states are needed for quantization\n        norm_hidden_states = self.layer_norm(hidden_states)\n        hidden_states = self.projection(norm_hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        return hidden_states\n\n\nclass SeamlessM4TConformerFeedForward(nn.Module):\n    def __init__(self, config, act_fn=None, dropout=None):\n        super().__init__()\n        dropout = dropout if dropout is not None else config.speech_encoder_dropout\n        act_fn = act_fn if act_fn is not None else config.speech_encoder_hidden_act\n\n        self.intermediate_dropout = nn.Dropout(dropout)\n        self.intermediate_dense = nn.Linear(config.hidden_size, config.speech_encoder_intermediate_size)\n        self.intermediate_act_fn = ACT2FN[act_fn] if isinstance(act_fn, str) else act_fn\n\n        self.output_dense = nn.Linear(config.speech_encoder_intermediate_size, config.hidden_size)\n        self.output_dropout = nn.Dropout(dropout)\n\n    def forward(self, hidden_states):\n        hidden_states = self.intermediate_dense(hidden_states)\n        hidden_states = self.intermediate_act_fn(hidden_states)\n        hidden_states = self.intermediate_dropout(hidden_states)\n\n        hidden_states = self.output_dense(hidden_states)\n        hidden_states = self.output_dropout(hidden_states)\n        return hidden_states\n\n\nclass SeamlessM4TConformerConvolutionModule(nn.Module):\n    \"\"\"Convolution block used in the conformer block\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        if (config.conv_depthwise_kernel_size - 1) % 2 == 1:\n            raise ValueError(\"`config.conv_depthwise_kernel_size` should be a odd number for 'SAME' padding\")\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n        self.pointwise_conv1 = nn.Conv1d(\n            config.hidden_size,\n            2 * config.hidden_size,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False,\n        )\n        self.glu = nn.GLU(dim=1)\n        self.depthwise_conv = nn.Conv1d(\n            config.hidden_size,\n            config.hidden_size,\n            config.conv_depthwise_kernel_size,\n            stride=1,\n            padding=\"same\",\n            groups=config.hidden_size,\n            bias=False,\n        )\n        self.batch_norm = nn.BatchNorm1d(config.hidden_size)\n        self.activation = ACT2FN[config.speech_encoder_hidden_act]\n        self.pointwise_conv2 = nn.Conv1d(\n            config.hidden_size,\n            config.hidden_size,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=False,\n        )\n        self.dropout = nn.Dropout(config.speech_encoder_dropout)\n\n    def forward(self, hidden_states, attention_mask=None):\n        hidden_states = self.layer_norm(hidden_states)\n\n        # Ensure that we do not leak padded positions in depthwise convolution.\n        # Put 0 where necessary\n        if attention_mask is not None:\n            hidden_states = hidden_states.masked_fill(~attention_mask.bool().unsqueeze(-1), 0.0)\n\n        # exchange the temporal dimension and the feature dimension\n        hidden_states = hidden_states.transpose(1, 2)\n\n        # GLU mechanism\n        # => (batch, 2*channel, dim)\n        hidden_states = self.pointwise_conv1(hidden_states)\n        # => (batch, channel, dim)\n        hidden_states = self.glu(hidden_states)\n\n        # 1D Depthwise Conv\n        hidden_states = self.depthwise_conv(hidden_states)\n        hidden_states = self.batch_norm(hidden_states)\n        hidden_states = self.activation(hidden_states)\n\n        hidden_states = self.pointwise_conv2(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\nclass SeamlessM4TConformerSelfAttention(nn.Module):\n    \"\"\"Construct a SeamlessM4TConformerSelfAttention object.\n    Can be enhanced with rotary or relative position embeddings.\n    \"\"\"\n\n    def __init__(self, config, use_position_embeddings=True):\n        super().__init__()\n\n        self.head_size = config.hidden_size // config.speech_encoder_attention_heads\n        self.num_heads = config.speech_encoder_attention_heads\n        self.position_embeddings_type = config.position_embeddings_type if use_position_embeddings else None\n\n        self.linear_q = nn.Linear(config.hidden_size, config.hidden_size)\n        self.linear_k = nn.Linear(config.hidden_size, config.hidden_size)\n        self.linear_v = nn.Linear(config.hidden_size, config.hidden_size)\n        self.linear_out = nn.Linear(config.hidden_size, config.hidden_size)\n\n        self.dropout = nn.Dropout(p=config.speech_encoder_dropout)\n\n        if self.position_embeddings_type == \"relative\":\n            # linear transformation for positional encoding\n            self.linear_pos = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n            # these two learnable bias are used in matrix c and matrix d\n            # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n            self.pos_bias_u = nn.Parameter(torch.zeros(self.num_heads, self.head_size))\n            self.pos_bias_v = nn.Parameter(torch.zeros(self.num_heads, self.head_size))\n\n    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention.forward\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        relative_position_embeddings: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        # self-attention mechanism\n        batch_size, sequence_length, hidden_size = hidden_states.size()\n\n        # make sure query/key states can be != value states\n        query_key_states = hidden_states\n        value_states = hidden_states\n\n        if self.position_embeddings_type == \"rotary\":\n            if relative_position_embeddings is None:\n                raise ValueError(\n                    \"`relative_position_embeddings` has to be defined when `self.position_embeddings_type == 'rotary'\"\n                )\n            query_key_states = self._apply_rotary_embedding(query_key_states, relative_position_embeddings)\n\n        # project query_key_states and value_states\n        query = self.linear_q(query_key_states).view(batch_size, -1, self.num_heads, self.head_size)\n        key = self.linear_k(query_key_states).view(batch_size, -1, self.num_heads, self.head_size)\n        value = self.linear_v(value_states).view(batch_size, -1, self.num_heads, self.head_size)\n\n        # => (batch, head, time1, d_k)\n        query = query.transpose(1, 2)\n        key = key.transpose(1, 2)\n        value = value.transpose(1, 2)\n\n        if self.position_embeddings_type == \"relative\":\n            if relative_position_embeddings is None:\n                raise ValueError(\n                    \"`relative_position_embeddings` has to be defined when `self.position_embeddings_type ==\"\n                    \" 'relative'\"\n                )\n            # apply relative_position_embeddings to qk scores\n            # as proposed in Transformer_XL: https://arxiv.org/abs/1901.02860\n            scores = self._apply_relative_embeddings(\n                query=query, key=key, relative_position_embeddings=relative_position_embeddings\n            )\n        else:\n            scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_size)\n\n        # apply attention_mask if necessary\n        if attention_mask is not None:\n            scores = scores + attention_mask\n\n        # => (batch, head, time1, time2)\n        probs = torch.softmax(scores, dim=-1)\n        probs = self.dropout(probs)\n\n        # => (batch, head, time1, d_k)\n        hidden_states = torch.matmul(probs, value)\n\n        # => (batch, time1, hidden_size)\n        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, self.num_heads * self.head_size)\n        hidden_states = self.linear_out(hidden_states)\n\n        return hidden_states, probs\n\n    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_rotary_embedding\n    def _apply_rotary_embedding(self, hidden_states, relative_position_embeddings):\n        batch_size, sequence_length, hidden_size = hidden_states.size()\n        hidden_states = hidden_states.view(batch_size, sequence_length, self.num_heads, self.head_size)\n\n        cos = relative_position_embeddings[0, :sequence_length, ...]\n        sin = relative_position_embeddings[1, :sequence_length, ...]\n\n        # rotate hidden_states with rotary embeddings\n        hidden_states = hidden_states.transpose(0, 1)\n        rotated_states_begin = hidden_states[..., : self.head_size // 2]\n        rotated_states_end = hidden_states[..., self.head_size // 2 :]\n        rotated_states = torch.cat((-rotated_states_end, rotated_states_begin), dim=rotated_states_begin.ndim - 1)\n        hidden_states = (hidden_states * cos) + (rotated_states * sin)\n        hidden_states = hidden_states.transpose(0, 1)\n\n        hidden_states = hidden_states.view(batch_size, sequence_length, self.num_heads * self.head_size)\n\n        return hidden_states\n\n    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerSelfAttention._apply_relative_embeddings\n    def _apply_relative_embeddings(self, query, key, relative_position_embeddings):\n        # 1. project positional embeddings\n        # => (batch, head, 2*time1-1, d_k)\n        proj_relative_position_embeddings = self.linear_pos(relative_position_embeddings)\n        proj_relative_position_embeddings = proj_relative_position_embeddings.view(\n            relative_position_embeddings.size(0), -1, self.num_heads, self.head_size\n        )\n        proj_relative_position_embeddings = proj_relative_position_embeddings.transpose(1, 2)\n        proj_relative_position_embeddings = proj_relative_position_embeddings.transpose(2, 3)\n\n        # 2. Add bias to query\n        # => (batch, head, time1, d_k)\n        query = query.transpose(1, 2)\n        q_with_bias_u = (query + self.pos_bias_u).transpose(1, 2)\n        q_with_bias_v = (query + self.pos_bias_v).transpose(1, 2)\n\n        # 3. attention score: first compute matrix a and matrix c\n        # as described in https://arxiv.org/abs/1901.02860 Section 3.3\n        # => (batch, head, time1, time2)\n        scores_ac = torch.matmul(q_with_bias_u, key.transpose(-2, -1))\n\n        # 4. then compute matrix b and matrix d\n        # => (batch, head, time1, 2*time1-1)\n        scores_bd = torch.matmul(q_with_bias_v, proj_relative_position_embeddings)\n\n        # 5. shift matrix b and matrix d\n        zero_pad = torch.zeros((*scores_bd.size()[:3], 1), device=scores_bd.device, dtype=scores_bd.dtype)\n        scores_bd_padded = torch.cat([zero_pad, scores_bd], dim=-1)\n        scores_bd_padded_shape = scores_bd.size()[:2] + (scores_bd.shape[3] + 1, scores_bd.shape[2])\n        scores_bd_padded = scores_bd_padded.view(*scores_bd_padded_shape)\n        scores_bd = scores_bd_padded[:, :, 1:].view_as(scores_bd)\n        scores_bd = scores_bd[:, :, :, : scores_bd.size(-1) // 2 + 1]\n\n        # 6. sum matrices\n        # => (batch, head, time1, time2)\n        scores = (scores_ac + scores_bd) / math.sqrt(self.head_size)\n\n        return scores\n\n\nclass SeamlessM4TConformerEncoderLayer(nn.Module):\n    \"\"\"Conformer block based on https://arxiv.org/abs/2005.08100.\"\"\"\n\n    # Copied from transformers.models.wav2vec2_conformer.modeling_wav2vec2_conformer.Wav2Vec2ConformerEncoderLayer.__init__ with Wav2Vec2->SeamlessM4T, attention_dropout->speech_encoder_dropout, torch.nn->nn\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.hidden_size\n        dropout = config.speech_encoder_dropout\n\n        # Feed-forward 1\n        self.ffn1_layer_norm = nn.LayerNorm(embed_dim)\n        self.ffn1 = SeamlessM4TConformerFeedForward(config)\n\n        # Self-Attention\n        self.self_attn_layer_norm = nn.LayerNorm(embed_dim)\n        self.self_attn_dropout = nn.Dropout(dropout)\n        self.self_attn = SeamlessM4TConformerSelfAttention(config)\n\n        # Conformer Convolution\n        self.conv_module = SeamlessM4TConformerConvolutionModule(config)\n\n        # Feed-forward 2\n        self.ffn2_layer_norm = nn.LayerNorm(embed_dim)\n        self.ffn2 = SeamlessM4TConformerFeedForward(config)\n        self.final_layer_norm = nn.LayerNorm(embed_dim)\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask: Optional[torch.Tensor] = None,\n        relative_position_embeddings: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n        conv_attention_mask: Optional[torch.Tensor] = None,\n    ):\n        hidden_states = hidden_states\n\n        # 1. Feed-Forward 1 layer\n        residual = hidden_states\n        hidden_states = self.ffn1_layer_norm(hidden_states)\n        hidden_states = self.ffn1(hidden_states)\n        hidden_states = hidden_states * 0.5 + residual\n        residual = hidden_states\n\n        # 2. Self-Attention layer\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states, attn_weigts = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            relative_position_embeddings=relative_position_embeddings,\n            output_attentions=output_attentions,\n        )\n        hidden_states = self.self_attn_dropout(hidden_states)\n        hidden_states = hidden_states + residual\n\n        # 3. Convolutional Layer\n        residual = hidden_states\n        hidden_states = self.conv_module(hidden_states, attention_mask=conv_attention_mask)\n        hidden_states = residual + hidden_states\n\n        # 4. Feed-Forward 2 Layer\n        residual = hidden_states\n        hidden_states = self.ffn2_layer_norm(hidden_states)\n        hidden_states = self.ffn2(hidden_states)\n        hidden_states = hidden_states * 0.5 + residual\n        hidden_states = self.final_layer_norm(hidden_states)\n\n        return hidden_states, attn_weigts\n\n\nclass SeamlessM4TConformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n\n        if config.position_embeddings_type == \"relative\":\n            self.embed_positions = SeamlessM4TConformerRelPositionalEmbedding(config)\n        elif config.position_embeddings_type == \"rotary\":\n            self.embed_positions = SeamlessM4TConformerRotaryPositionalEmbedding(config)\n        else:\n            self.embed_positions = None\n\n        self.dropout = nn.Dropout(config.speech_encoder_dropout)\n        self.layers = nn.ModuleList(\n            [SeamlessM4TConformerEncoderLayer(config) for _ in range(config.speech_encoder_layers)]\n        )\n\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n\n        self.gradient_checkpointing = False\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask=None,\n        output_attentions=False,\n        output_hidden_states=False,\n        return_dict=True,\n    ):\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attentions = () if output_attentions else None\n\n        conv_attention_mask = attention_mask\n        if attention_mask is not None:\n            # make sure padded tokens output 0\n            hidden_states = hidden_states.masked_fill(~attention_mask.bool().unsqueeze(-1), 0.0)\n            # extend attention_mask\n            attention_mask = 1.0 - attention_mask[:, None, None, :].to(dtype=hidden_states.dtype)\n            attention_mask = attention_mask * torch.finfo(hidden_states.dtype).min\n            attention_mask = attention_mask.expand(\n                attention_mask.shape[0], 1, attention_mask.shape[-1], attention_mask.shape[-1]\n            )\n\n        hidden_states = self.dropout(hidden_states)\n\n        if self.embed_positions is not None:\n            relative_position_embeddings = self.embed_positions(hidden_states)\n        else:\n            relative_position_embeddings = None\n\n        deepspeed_zero3_is_enabled = is_deepspeed_zero3_enabled()\n\n        for i, layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states = all_hidden_states + (hidden_states,)\n\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            dropout_probability = torch.rand([])\n\n            skip_the_layer = (\n                True if self.training and (dropout_probability < self.config.speech_encoder_layerdrop) else False\n            )\n            if not skip_the_layer or deepspeed_zero3_is_enabled:\n                # under deepspeed zero3 all gpus must run in sync\n                if self.gradient_checkpointing and self.training:\n                    layer_outputs = self._gradient_checkpointing_func(\n                        layer.__call__,\n                        hidden_states,\n                        attention_mask,\n                        relative_position_embeddings,\n                    )\n                else:\n                    layer_outputs = layer(\n                        hidden_states,\n                        attention_mask=attention_mask,\n                        relative_position_embeddings=relative_position_embeddings,\n                        output_attentions=output_attentions,\n                        conv_attention_mask=conv_attention_mask,\n                    )\n                hidden_states = layer_outputs[0]\n\n            if skip_the_layer:\n                layer_outputs = (None, None)\n\n            if output_attentions:\n                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n\n        hidden_states = self.layer_norm(hidden_states)\n        if output_hidden_states:\n            all_hidden_states = all_hidden_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attentions,\n        )\n\n\nclass SeamlessM4TConformerAdapterLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.hidden_size\n        dropout = config.adaptor_dropout\n\n        self.kernel_size = config.adaptor_kernel_size\n        self.stride = config.adaptor_stride\n\n        # 1. residual convolution\n        self.residual_layer_norm = nn.LayerNorm(embed_dim)\n        self.residual_conv = nn.Conv1d(\n            embed_dim,\n            2 * embed_dim,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.stride // 2,\n        )\n        self.activation = nn.GLU(dim=1)\n\n        # Self-Attention\n        self.self_attn_layer_norm = nn.LayerNorm(embed_dim)\n        self.self_attn_conv = nn.Conv1d(\n            embed_dim,\n            2 * embed_dim,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.stride // 2,\n        )\n        self.self_attn = SeamlessM4TConformerSelfAttention(config, use_position_embeddings=False)\n        self.self_attn_dropout = nn.Dropout(dropout)\n\n        # Feed-forward\n        self.ffn_layer_norm = nn.LayerNorm(embed_dim)\n        self.ffn = SeamlessM4TConformerFeedForward(config, act_fn=\"relu\", dropout=dropout)\n\n    def _compute_sub_sample_lengths_from_attention_mask(self, attention_mask):\n        pad = self.kernel_size // 2\n        seq_lens = attention_mask.size(1) - (1 - attention_mask.int()).sum(1)\n\n        seq_lens = ((seq_lens + 2 * pad - self.kernel_size) / self.stride) + 1\n\n        return seq_lens.floor()\n\n    def forward(\n        self,\n        hidden_states,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ):\n        residual = self.residual_layer_norm(hidden_states)\n\n        # Apply pooling to the residual to match the sequence length of the\n        # multi-head attention output.\n        # (batch, seq_len, feature_dim) -> (batch, feature_dim, seq_len)\n        residual = residual.transpose(1, 2)\n        residual = self.residual_conv(residual)\n        residual = self.activation(residual)\n        # (batch, feature_dim, seq_len) -> (batch, seq_len, feature_dim)\n        residual = residual.transpose(1, 2)\n\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n        # Apply pooling before feeding to the multihead-attention layer.\n        # (batch, seq_len, feature_dim) -> (batch, feature_dim, seq_len)\n        hidden_states = hidden_states.transpose(1, 2)\n        hidden_states = self.self_attn_conv(hidden_states)\n        hidden_states = self.activation(hidden_states)\n        # (batch, feature_dim, seq_len) -> (batch, seq_len, feature_dim)\n        hidden_states = hidden_states.transpose(1, 2)\n\n        if attention_mask is not None:\n            sub_sampled_lengths = self._compute_sub_sample_lengths_from_attention_mask(attention_mask).to(\n                hidden_states.device\n            )\n            attention_mask = _compute_new_attention_mask(hidden_states=hidden_states, seq_lens=sub_sampled_lengths)\n            attention_mask = _prepare_4d_attention_mask(\n                attention_mask,\n                hidden_states.dtype,\n            )\n\n        # The rest of the computation is identical to a vanilla Transformer\n        # encoder layer.\n        hidden_states, attn_weigths = self.self_attn(\n            hidden_states,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = self.self_attn_dropout(hidden_states)\n        hidden_states = hidden_states + residual\n\n        residual = hidden_states\n\n        hidden_states = self.ffn_layer_norm(hidden_states)\n        hidden_states = self.ffn(hidden_states) + residual\n\n        return hidden_states\n\n\nclass SeamlessM4TConformerAdapter(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        self.layers = nn.ModuleList(SeamlessM4TConformerAdapterLayer(config) for _ in range(config.num_adapter_layers))\n\n    def forward(self, hidden_states, attention_mask):\n        # down project hidden_states if necessary\n\n        for layer in self.layers:\n            hidden_states = layer(hidden_states, attention_mask)\n\n        return hidden_states\n\n\n############ TEXT / UNITS related code ################\n\n\n# Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding\nclass SeamlessM4TSinusoidalPositionalEmbedding(nn.Module):\n    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n\n    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n        super().__init__()\n        self.offset = 2\n        self.embedding_dim = embedding_dim\n        self.padding_idx = padding_idx\n        self.make_weights(num_positions + self.offset, embedding_dim, padding_idx)\n\n    def make_weights(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):\n        emb_weights = self.get_embedding(num_embeddings, embedding_dim, padding_idx)\n        if hasattr(self, \"weights\"):\n            # in forward put the weights on the correct dtype and device of the param\n            emb_weights = emb_weights.to(dtype=self.weights.dtype, device=self.weights.device)\n\n        self.register_buffer(\"weights\", emb_weights, persistent=False)\n\n    @staticmethod\n    def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = None):\n        \"\"\"\n        Build sinusoidal embeddings.\n\n        This matches the implementation in tensor2tensor, but differs slightly from the description in Section 3.5 of\n        \"Attention Is All You Need\".\n        \"\"\"\n        half_dim = embedding_dim // 2\n        emb = math.log(10000) / (half_dim - 1)\n        emb = torch.exp(torch.arange(half_dim, dtype=torch.int64).float() * -emb)\n        emb = torch.arange(num_embeddings, dtype=torch.int64).float().unsqueeze(1) * emb.unsqueeze(0)\n        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n        if embedding_dim % 2 == 1:\n            # zero pad\n            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n        if padding_idx is not None:\n            emb[padding_idx, :] = 0\n\n        return emb.to(torch.get_default_dtype())\n\n    @torch.no_grad()\n    def forward(\n        self, input_ids: torch.Tensor = None, inputs_embeds: torch.Tensor = None, past_key_values_length: int = 0\n    ):\n        if input_ids is not None:\n            bsz, seq_len = input_ids.size()\n            # Create the position ids from the input token ids. Any padded tokens remain padded.\n            position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length).to(\n                input_ids.device\n            )\n        else:\n            bsz, seq_len = inputs_embeds.size()[:-1]\n            position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length)\n\n        # expand embeddings if needed\n        max_pos = self.padding_idx + 1 + seq_len + past_key_values_length\n        if max_pos > self.weights.size(0):\n            self.make_weights(max_pos + self.offset, self.embedding_dim, self.padding_idx)\n\n        return self.weights.index_select(0, position_ids.view(-1)).view(bsz, seq_len, self.weights.shape[-1]).detach()\n\n    def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_length):\n        \"\"\"\n        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n\n        Args:\n            inputs_embeds: torch.Tensor\n\n        Returns: torch.Tensor\n        \"\"\"\n        input_shape = inputs_embeds.size()[:-1]\n        sequence_length = input_shape[1]\n\n        position_ids = torch.arange(\n            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n        )\n        return position_ids.unsqueeze(0).expand(input_shape).contiguous() + past_key_values_length\n\n\nclass SeamlessM4TAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    # Copied from transformers.models.bart.modeling_bart.BartAttention.__init__ with Bart->SeamlessM4T\n    def __init__(\n        self,\n        embed_dim: int,\n        num_heads: int,\n        dropout: float = 0.0,\n        is_decoder: bool = False,\n        bias: bool = True,\n        is_causal: bool = False,\n        config: Optional[SeamlessM4TConfig] = None,\n    ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.dropout = dropout\n        self.head_dim = embed_dim // num_heads\n        self.config = config\n\n        if (self.head_dim * num_heads) != self.embed_dim:\n            raise ValueError(\n                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n                f\" and `num_heads`: {num_heads}).\"\n            )\n        self.scaling = self.head_dim**-0.5\n        self.is_decoder = is_decoder\n        self.is_causal = is_causal\n\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        # if encoder_hidden_states are provided this layer is used as a cross-attention layer\n        # for the decoder\n        is_cross_attention = encoder_hidden_states is not None\n\n        bsz, tgt_len, _ = hidden_states.size()\n\n        # get query proj\n        query_states = self.q_proj(hidden_states) * self.scaling\n        # get key, value proj\n        # `past_key_value[0].shape[2] == encoder_hidden_states.shape[1]`\n        # is checking that the `sequence_length` of the `past_key_value` is the same as\n        # the provided `encoder_hidden_states` to support prefix tuning\n        if (\n            is_cross_attention\n            and past_key_value is not None\n            and past_key_value[0].shape[2] == encoder_hidden_states.shape[1]\n        ):\n            # reuse k,v, cross_attentions\n            key_states = past_key_value[0]\n            value_states = past_key_value[1]\n        elif is_cross_attention:\n            # cross_attentions\n            key_states = self._shape(self.k_proj(encoder_hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(encoder_hidden_states), -1, bsz)\n        elif past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n        else:\n            # self_attention\n            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n\n        if self.is_decoder:\n            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n            # Further calls to cross_attention layer can then reuse all cross-attention\n            # key/value_states (first \"if\" case)\n            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n            # if encoder bi-directional self-attention `past_key_value` is always `None`\n            past_key_value = (key_states, value_states)\n\n        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n        key_states = key_states.reshape(*proj_shape)\n        value_states = value_states.reshape(*proj_shape)\n\n        src_len = key_states.size(1)\n        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n\n        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, tgt_len, src_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n\n        if output_attentions:\n            # this operation is a bit awkward, but it's required to\n            # make sure that attn_weights keeps its gradient.\n            # In order to do so, attn_weights have to be reshaped\n            # twice and have to be reused in the following\n            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n        else:\n            attn_weights_reshaped = None\n\n        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n\n        attn_output = torch.bmm(attn_probs, value_states)\n\n        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz * self.num_heads, tgt_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n        attn_output = attn_output.transpose(1, 2)\n\n        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n        # partitioned across GPUs when using tensor-parallelism.\n        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n\n        attn_output = self.out_proj(attn_output)\n\n        return attn_output, attn_weights_reshaped, past_key_value\n\n\n# Copied from transformers.models.nllb_moe.modeling_nllb_moe.NllbMoeDenseActDense with NllbMoe->SeamlessM4T,DenseActDense->FeedForwardNetwork, d_model->hidden_size\nclass SeamlessM4TFeedForwardNetwork(nn.Module):\n    def __init__(self, config: SeamlessM4TConfig, ffn_dim: int):\n        super().__init__()\n        self.fc1 = nn.Linear(config.hidden_size, ffn_dim)\n        self.fc2 = nn.Linear(ffn_dim, config.hidden_size)\n        self.dropout = nn.Dropout(config.activation_dropout)\n        self.act = ACT2FN[config.activation_function]\n\n    def forward(self, hidden_states):\n        hidden_states = self.fc1(hidden_states)\n        hidden_states = self.act(hidden_states)\n        hidden_states = self.dropout(hidden_states)\n        if (\n            isinstance(self.fc2.weight, torch.Tensor)\n            and hidden_states.dtype != self.fc2.weight.dtype\n            and (self.fc2.weight.dtype != torch.int8 and self.fc2.weight.dtype != torch.uint8)\n        ):\n            hidden_states = hidden_states.to(self.fc2.weight.dtype)\n        hidden_states = self.fc2(hidden_states)\n        return hidden_states\n\n\nclass SeamlessM4TEncoderLayer(nn.Module):\n    def __init__(self, config: SeamlessM4TConfig, encoder_ffn_dim=None, encoder_attention_heads=None):\n        super().__init__()\n        encoder_ffn_dim = config.encoder_ffn_dim if encoder_ffn_dim is None else encoder_ffn_dim\n        encoder_attention_heads = (\n            config.encoder_attention_heads if encoder_attention_heads is None else encoder_attention_heads\n        )\n\n        self.embed_dim = config.hidden_size\n        self.self_attn = SeamlessM4TAttention(\n            embed_dim=self.embed_dim,\n            num_heads=encoder_attention_heads,\n            dropout=config.attention_dropout,\n        )\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n\n        self.ffn = SeamlessM4TFeedForwardNetwork(config, ffn_dim=encoder_ffn_dim)\n\n        self.ffn_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.ffn_dropout = nn.Dropout(config.activation_dropout)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: torch.Tensor,\n        output_attentions: bool = False,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`):\n                input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`):\n                attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very\n                large negative values.\n        \"\"\"\n        residual = hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n        hidden_states, attn_weights, _ = self.self_attn(\n            hidden_states=hidden_states,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = self.attn_dropout(hidden_states)\n        hidden_states = residual + hidden_states\n\n        residual = hidden_states\n\n        hidden_states = self.ffn_layer_norm(hidden_states)\n\n        hidden_states = self.ffn(hidden_states)\n        hidden_states = self.ffn_dropout(hidden_states)\n\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (attn_weights,)\n\n        return outputs\n\n\nclass SeamlessM4TDecoderLayer(nn.Module):\n    def __init__(self, config: SeamlessM4TConfig, decoder_ffn_dim=None, decoder_attention_heads=None):\n        super().__init__()\n        decoder_ffn_dim = config.decoder_ffn_dim if decoder_ffn_dim is None else decoder_ffn_dim\n        decoder_attention_heads = (\n            config.decoder_attention_heads if decoder_attention_heads is None else decoder_attention_heads\n        )\n\n        self.embed_dim = config.hidden_size\n        self.self_attn = SeamlessM4TAttention(\n            embed_dim=self.embed_dim,\n            num_heads=decoder_attention_heads,\n            dropout=config.attention_dropout,\n            is_decoder=True,\n        )\n        self.dropout = config.dropout\n        self.activation_fn = ACT2FN[config.activation_function]\n        self.attn_dropout = nn.Dropout(config.dropout)\n\n        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n        self.cross_attention = SeamlessM4TAttention(\n            self.embed_dim, decoder_attention_heads, config.attention_dropout, is_decoder=True\n        )\n        self.cross_attention_layer_norm = nn.LayerNorm(self.embed_dim)\n\n        self.ffn = SeamlessM4TFeedForwardNetwork(config, ffn_dim=decoder_ffn_dim)\n\n        self.ffn_layer_norm = nn.LayerNorm(config.hidden_size)\n        self.ffn_dropout = nn.Dropout(config.activation_dropout)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = True,\n    ) -> torch.Tensor:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`):\n                input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`):\n                attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very\n                large negative values.\n            encoder_hidden_states (`torch.FloatTensor`):\n                cross attention input to the layer of shape `(batch, seq_len, embed_dim)`\n            encoder_attention_mask (`torch.FloatTensor`):\n                encoder attention mask of size `(batch, 1, tgt_len, src_len)` where padding elements are indicated by\n                very large negative values.\n            past_key_value (`Tuple(torch.FloatTensor)`):\n                cached past key and value projection states\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n        \"\"\"\n        residual = hidden_states\n        hidden_states = self.self_attn_layer_norm(hidden_states)\n\n        # Self Attention\n        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n        # add present self-attn cache to positions 1,2 of present_key_value tuple\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            past_key_value=self_attn_past_key_value,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n        )\n        hidden_states = self.attn_dropout(hidden_states)\n        hidden_states = residual + hidden_states\n\n        # Cross-Attention Block\n        cross_attn_present_key_value = None\n        cross_attn_weights = None\n        if encoder_hidden_states is not None:\n            residual = hidden_states\n            hidden_states = self.cross_attention_layer_norm(hidden_states)\n\n            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n\n            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.cross_attention(\n                hidden_states=hidden_states,\n                encoder_hidden_states=encoder_hidden_states,\n                past_key_value=cross_attn_past_key_value,\n                attention_mask=encoder_attention_mask,\n                output_attentions=output_attentions,\n            )\n            hidden_states = self.attn_dropout(hidden_states)\n            hidden_states = residual + hidden_states\n\n            # add cross-attn to positions 3,4 of present_key_value tuple\n            present_key_value += cross_attn_present_key_value\n\n        # Fully Connected\n        residual = hidden_states\n\n        hidden_states = self.ffn_layer_norm(hidden_states)\n\n        hidden_states = self.ffn(hidden_states)\n        hidden_states = self.ffn_dropout(hidden_states)\n\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states, present_key_value)\n\n        if output_attentions:\n            outputs += (self_attn_weights, cross_attn_weights)\n\n        return outputs\n\n\n############ SUB-MODELS related code ################\n\n\nclass SeamlessM4TPreTrainedModel(PreTrainedModel):\n    \"\"\"\n    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n    models.\n    \"\"\"\n\n    config_class = SeamlessM4TConfig\n    base_model_prefix = \"seamless_m4t\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"SeamlessM4TEncoderLayer\", \"SeamlessM4TDecoderLayer\", \"SeamlessM4TConformerEncoderLayer\"]\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights\"\"\"\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, SeamlessM4TConformerSelfAttention):\n            if hasattr(module, \"pos_bias_u\"):\n                nn.init.xavier_uniform_(module.pos_bias_u)\n            if hasattr(module, \"pos_bias_v\"):\n                nn.init.xavier_uniform_(module.pos_bias_v)\n        elif isinstance(module, SeamlessM4TConformerPositionalConvEmbedding):\n            nn.init.normal_(\n                module.conv.weight,\n                mean=0,\n                std=2 * math.sqrt(1 / (module.conv.kernel_size[0] * module.conv.in_channels)),\n            )\n            nn.init.constant_(module.conv.bias, 0)\n        elif isinstance(module, SeamlessM4TConformerFeatureProjection):\n            k = math.sqrt(1 / module.projection.in_features)\n            nn.init.uniform_(module.projection.weight, a=-k, b=k)\n            nn.init.uniform_(module.projection.bias, a=-k, b=k)\n        elif isinstance(module, (nn.LayerNorm, nn.GroupNorm)):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        elif isinstance(module, nn.Conv1d):\n            nn.init.kaiming_normal_(module.weight)\n            if module.bias is not None:\n                k = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                nn.init.uniform_(module.bias, a=-k, b=k)\n\n    def _compute_sub_sample_lengths_from_attention_mask(self, attention_mask):\n        kernel_size, stride = self.config.adaptor_kernel_size, self.config.adaptor_stride\n        pad = kernel_size // 2\n        seq_lens = attention_mask.size(1) - (1 - attention_mask.int()).sum(1)\n\n        seq_lens = ((seq_lens + 2 * pad - kernel_size) / stride) + 1\n\n        return seq_lens.floor()\n\n    def compute_last_hidden_states_per_sample(\n        self,\n        hidden_states: Tuple[Tuple[torch.Tensor]],\n        beam_indices: Optional[torch.Tensor] = None,\n    ) -> torch.Tensor:\n        \"\"\"\n        Computes the last hidden states.\n\n        Parameters:\n            hidden_states (`Tuple[Tuple[torch.Tensor]]`):\n                The generated hidden states. Tuple (one element for each generated token) of tuples (one element for\n                each layer of the decoder) of torch.FloatTensor of shape (batch_size*num_beams*num_return_sequences,\n                generated_length, hidden_size).\n            beam_indices (`torch.LongTensor`, *optional*):\n                Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n                `(batch_size*num_return_sequences, sequence_length)`. Only required if a `num_beams>1` at\n                generate-time.\n\n        Return:\n            `torch.Tensor`: A `torch.Tensor` of shape `(batch_size*num_return_sequences, sequence_length, hidden_size)`\n            containing\n                the last hidden states.\n        ```\"\"\"\n        # 1. First, let's compute last_hidden_states from hidden_states.\n        # For each generation step, takes the hidden state from the last layer.\n        # shape: (batch_size*vocab_size*num_return_sequences, # generation_steps, hidden_dim)\n        last_hidden_states = torch.concat([hidden_states[-1] for hidden_states in hidden_states], dim=1)\n\n        # 2. In absence of `beam_indices`, we can assume that we come from e.g. greedy search, which is equivalent\n        # to a beam search approach were the first (and only) beam is always selected\n        # in that case, return directly last_hidden_states\n        if beam_indices is None:\n            return last_hidden_states\n\n        # 3. cut beam_indices to longest beam length\n        beam_indices_mask = beam_indices < 0\n        max_beam_length = (1 - beam_indices_mask.long()).sum(-1).max()\n        beam_indices = beam_indices.clone()[:, :max_beam_length]\n        beam_indices_mask = beam_indices_mask[:, :max_beam_length]\n\n        # 4. Set indices of beams that finished early to 0; such indices will be masked correctly afterwards anyways\n        beam_indices[beam_indices_mask] = 0\n\n        # 5. expand beam_indices to last_hidden_states dim\n        beam_indices = beam_indices.unsqueeze(-1)\n        beam_indices = beam_indices.expand(-1, -1, last_hidden_states.shape[-1])\n\n        # 6. select the right candidate for each beam\n        # in other words, new_last_hidden_states[i,j,k] = last_hidden_states[beam_indices[i,j,k], j, k] for all i, j, k\n        last_hidden_states = torch.gather(last_hidden_states, 0, beam_indices)\n\n        return last_hidden_states\n\n\n@add_start_docstrings(\n    \"\"\"Transformer speech encoder consisting of *config.speech_encoder_layers* conformer self attention layers.\n    Each layer is a [`SeamlessM4TConformerEncoderLayer`].\"\"\",\n    SEAMLESS_M4T_START_DOCSTRING,\n)\nclass SeamlessM4TSpeechEncoder(SeamlessM4TPreTrainedModel):\n    main_input_name = \"input_features\"\n\n    def __init__(self, config: SeamlessM4TConfig):\n        super().__init__(config)\n\n        self.feature_projection = SeamlessM4TConformerFeatureProjection(config)\n        self.encoder = SeamlessM4TConformerEncoder(config)\n        self.intermediate_ffn = SeamlessM4TConformerFeedForward(config, act_fn=\"relu\", dropout=0.0)\n        self.adapter = SeamlessM4TConformerAdapter(config) if config.add_adapter else None\n        self.inner_layer_norm = nn.LayerNorm(config.hidden_size)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_features: Optional[torch.Tensor],\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ) -> Union[Tuple, Wav2Vec2BaseModelOutput]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if input_features is None:\n            raise ValueError(\n                \"\"\"Both `input_features` and `inputs_embeds` are `None` in `SeamlessM4TSpeechEncoder.forward`.\n                Make sure one of them is not `None`.\"\"\"\n            )\n\n        hidden_states = self.feature_projection(input_features)\n\n        encoder_outputs = self.encoder(\n            hidden_states,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = encoder_outputs[0]\n\n        expanded_hidden_states = self.intermediate_ffn(hidden_states)\n        hidden_states = hidden_states + 0.5 * expanded_hidden_states\n\n        if self.adapter is not None:\n            hidden_states = self.adapter(hidden_states, attention_mask=attention_mask)\n\n        hidden_states = self.inner_layer_norm(hidden_states)\n\n        if not return_dict:\n            return (hidden_states,) + encoder_outputs[1:]\n\n        return Wav2Vec2BaseModelOutput(\n            last_hidden_state=hidden_states,\n            hidden_states=encoder_outputs.hidden_states,\n            attentions=encoder_outputs.attentions,\n        )\n\n\n# inspired from MBart and NllbMoe\n@add_start_docstrings(\n    \"Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a [`SeamlessM4TEncoderLayer`].\",\n    SEAMLESS_M4T_START_DOCSTRING,\n    \"\"\"\n        embed_tokens (`nn.Embedding`, *optional*):\n            Input embedding\n        is_t2u_encoder (`bool`, *optional*, defaults to `False`):\n            indicates if it belongs to the text-to-units model, in which case it won't have input embeddings\n    \"\"\",\n)\nclass SeamlessM4TEncoder(SeamlessM4TPreTrainedModel):\n    def __init__(\n        self,\n        config: SeamlessM4TConfig,\n        embed_tokens: Optional[nn.Embedding] = None,\n        is_t2u_encoder: bool = False,\n    ):\n        super().__init__(config)\n\n        self.dropout = config.dropout\n        self.layerdrop = config.encoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        embed_dim = config.hidden_size\n\n        self.is_t2u_encoder = is_t2u_encoder\n        self.max_source_positions = config.max_position_embeddings\n\n        if not self.is_t2u_encoder:\n            self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n\n            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n\n            if embed_tokens is not None:\n                self.embed_tokens.weight = embed_tokens.weight\n\n            self.embed_positions = SeamlessM4TSinusoidalPositionalEmbedding(\n                self.max_source_positions,\n                embed_dim,\n                self.padding_idx,\n            )\n\n        layers = []\n        for _ in range(config.encoder_layers):\n            layers.append(\n                SeamlessM4TEncoderLayer(\n                    config,\n                    encoder_attention_heads=config.encoder_attention_heads,\n                    encoder_ffn_dim=config.encoder_ffn_dim,\n                )\n            )\n\n        self.layers = nn.ModuleList(layers)\n\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ) -> Union[Tuple, BaseModelOutput]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if input_ids is not None and self.is_t2u_encoder:\n            raise ValueError(\n                \"You cannot pass input_ids to the encoder of the text_to_units model. Pass inputs_embeds instead.\"\n            )\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input = input_ids\n            input_shape = input.shape\n            input_ids = input_ids.view(-1, input_shape[-1])\n        elif inputs_embeds is not None:\n            input = inputs_embeds[:, :, -1]\n        else:\n            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n\n        if not self.is_t2u_encoder:\n            embed_pos = self.embed_positions(input)\n\n            hidden_states = inputs_embeds + embed_pos.to(inputs_embeds.device)\n        else:\n            hidden_states = inputs_embeds\n\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n        # expand attention_mask\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n\n        encoder_states = () if output_hidden_states else None\n        all_attentions = () if output_attentions else None\n\n        for idx, encoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                encoder_states = encoder_states + (hidden_states,)\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            to_drop = False\n            if self.training:\n                dropout_probability = torch.rand([])\n                if dropout_probability < self.layerdrop:  # skip the layer\n                    to_drop = True\n\n            if to_drop:\n                layer_outputs = (None, None)\n            else:\n                if self.gradient_checkpointing and self.training:\n                    layer_outputs = self._gradient_checkpointing_func(\n                        encoder_layer.forward,\n                        hidden_states,\n                        attention_mask,\n                        output_attentions,\n                    )\n                else:\n                    layer_outputs = encoder_layer(\n                        hidden_states,\n                        attention_mask,\n                        output_attentions=output_attentions,\n                    )\n\n                hidden_states = layer_outputs[0]\n\n            if output_attentions:\n                all_attentions = all_attentions + (layer_outputs[1],)\n\n        hidden_states = self.layer_norm(hidden_states)\n\n        if output_hidden_states:\n            encoder_states = encoder_states + (hidden_states,)\n\n        if not return_dict:\n            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n        return BaseModelOutput(\n            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n        )\n\n\n@add_start_docstrings(\n    \"Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`SeamlessM4TDecoderLayer`].\",\n    SEAMLESS_M4T_START_DOCSTRING,\n    \"\"\"\n        embed_tokens (`nn.Embedding`, *optional*):\n            Input embedding\n    \"\"\",\n)\nclass SeamlessM4TDecoder(SeamlessM4TPreTrainedModel):\n    def __init__(\n        self,\n        config: SeamlessM4TConfig,\n        embed_tokens: Optional[nn.Embedding] = None,\n    ):\n        super().__init__(config)\n        self.dropout = config.dropout\n        self.layerdrop = config.decoder_layerdrop\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n        self.max_target_positions = config.max_position_embeddings\n        self.embed_scale = math.sqrt(config.hidden_size) if config.scale_embedding else 1.0\n\n        if embed_tokens is not None:\n            # if embed_tokens defined, use its shape instead\n            self.embed_tokens = nn.Embedding(embed_tokens.num_embeddings, embed_tokens.embedding_dim, self.padding_idx)\n            self.embed_tokens.weight = embed_tokens.weight\n        else:\n            self.embed_tokens = nn.Embedding(self.vocab_size, config.hidden_size, self.padding_idx)\n\n        self.embed_positions = SeamlessM4TSinusoidalPositionalEmbedding(\n            self.max_target_positions,\n            config.hidden_size,\n            padding_idx=self.padding_idx,\n        )\n\n        layers = []\n        for _ in range(config.decoder_layers):\n            layers.append(\n                SeamlessM4TDecoderLayer(\n                    config,\n                    decoder_attention_heads=config.decoder_attention_heads,\n                    decoder_ffn_dim=config.decoder_ffn_dim,\n                )\n            )\n        self.layers = nn.ModuleList(layers)\n        self.layer_norm = nn.LayerNorm(config.hidden_size)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n        encoder_attention_mask: Optional[torch.LongTensor] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n                of the decoder.\n            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n                selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            input = input_ids\n            input_shape = input.size()\n            input_ids = input_ids.view(-1, input_shape[-1])\n        elif inputs_embeds is not None:\n            input_shape = inputs_embeds.size()[:-1]\n            input = inputs_embeds[:, :, -1]\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n\n        # past_key_values_length\n        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n\n        attention_mask = _prepare_4d_causal_attention_mask(\n            attention_mask, input_shape, inputs_embeds, past_key_values_length\n        )\n\n        # expand encoder attention mask\n        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            encoder_attention_mask = _prepare_4d_attention_mask(\n                encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n            )\n\n        # embed positions\n        positions = self.embed_positions(input, past_key_values_length=past_key_values_length)\n\n        hidden_states = inputs_embeds + positions.to(inputs_embeds.device)\n\n        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing`. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n            if self.training:\n                dropout_probability = torch.rand([])\n                if dropout_probability < self.layerdrop:\n                    continue\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n                layer_outputs = self._gradient_checkpointing_func(\n                    decoder_layer.__call__,\n                    hidden_states,\n                    attention_mask,\n                    encoder_hidden_states,\n                    encoder_attention_mask,\n                    None,\n                    output_attentions,\n                    use_cache,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    encoder_hidden_states=encoder_hidden_states,\n                    encoder_attention_mask=encoder_attention_mask,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[2],)\n\n                if encoder_hidden_states is not None:\n                    all_cross_attentions += (layer_outputs[3],)\n\n        hidden_states = self.layer_norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(\n                v\n                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n                if v is not None\n            )\n        return BaseModelOutputWithPastAndCrossAttentions(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n            cross_attentions=all_cross_attentions,\n        )\n\n\n@add_start_docstrings(\n    \"Transformer bare text-to-unit encoder-decoder. The encoder is a [`SeamlessM4TEncoder`] without embeddings and the decoder is a [`SeamlessM4TDecoder`].\",\n    SEAMLESS_M4T_START_DOCSTRING,\n    \"\"\"\n        embed_tokens_decoder (`nn.Embedding`, *optional*): input embedding of the decoder.\n    \"\"\",\n)\nclass SeamlessM4TTextToUnitModel(SeamlessM4TPreTrainedModel):\n    def __init__(\n        self,\n        config: SeamlessM4TConfig,\n        embed_tokens_decoder: Optional[nn.Embedding] = None,\n    ):\n        super().__init__(config)\n\n        self.encoder = SeamlessM4TEncoder(config, is_t2u_encoder=True)\n        self.decoder = SeamlessM4TDecoder(config, embed_tokens_decoder)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple[torch.Tensor], Seq2SeqModelOutput]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if encoder_outputs is None:\n            encoder_outputs = self.encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n        decoder_outputs = self.decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            encoder_attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        if not return_dict:\n            return decoder_outputs + encoder_outputs\n\n        return Seq2SeqModelOutput(\n            last_hidden_state=decoder_outputs.last_hidden_state,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n\n@add_start_docstrings(\n    \"Transformer text-to-unit encoder-decoder with a language model head. The base encoder-decoder model is a [`SeamlessM4TTextToUnit`].\",\n    SEAMLESS_M4T_START_DOCSTRING,\n    \"\"\"\n        embed_tokens_decoder (`nn.Embedding`, *optional*): input embedding of the decoder.\n    \"\"\",\n)\nclass SeamlessM4TTextToUnitForConditionalGeneration(SeamlessM4TPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\n        \"vocoder\",\n        \"speech_encoder\",\n        \"text_encoder\",\n        \"text_decoder\",\n    ]\n    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n\n    def __init__(\n        self,\n        config: SeamlessM4TConfig,\n        embed_tokens_decoder: Optional[nn.Embedding] = None,\n    ):\n        # update config - used principaly for bos_token_id etc.\n        config = copy.deepcopy(config)\n        for param, val in config.to_dict().items():\n            if param.startswith(\"t2u_\"):\n                config.__setattr__(param[4:], val)\n        super().__init__(config)\n\n        self.model = SeamlessM4TTextToUnitModel(config, embed_tokens_decoder)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.t2u_vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_encoder(self):\n        return self.model.encoder\n\n    def get_decoder(self):\n        return self.model.decoder\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_input_embeddings(self):\n        return self.model.decoder.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.decoder.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(M4T_TEXT_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Seq2SeqLMOutput, Tuple[torch.FloatTensor]]:\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if labels is not None:\n            if use_cache:\n                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n            use_cache = False\n            if decoder_input_ids is None and decoder_inputs_embeds is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.t2u_pad_token_id, self.config.t2u_decoder_start_token_id\n                )\n\n        outputs = self.model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            encoder_outputs=encoder_outputs,\n            decoder_attention_mask=decoder_attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            decoder_inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        lm_logits = self.lm_head(outputs[0])\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            labels = labels.to(lm_logits.device)\n            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            output = (lm_logits,) + outputs[1:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,\n            logits=lm_logits,\n            past_key_values=outputs.past_key_values,\n            decoder_hidden_states=outputs.decoder_hidden_states,\n            decoder_attentions=outputs.decoder_attentions,\n            cross_attentions=outputs.cross_attentions,\n            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n            encoder_hidden_states=outputs.encoder_hidden_states,\n            encoder_attentions=outputs.encoder_attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # cut decoder_input_ids if past is used\n        if past_key_values is not None:\n            decoder_input_ids = decoder_input_ids[:, -1:]\n\n        return {\n            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past_key_values,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"use_cache\": use_cache,\n        }\n\n    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n        return shift_tokens_right(labels, self.config.t2u_pad_token_id, self.config.t2u_decoder_start_token_id)\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            # cached cross_attention states don't have to be reordered -> they are always the same\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n            )\n        return reordered_past\n\n    def _tie_weights(self) -> None:\n        if getattr(self.config, \"tie_word_embeddings\", True):\n            output_embeddings = self.get_output_embeddings()\n            if output_embeddings is not None:\n                self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n\n\n############ VOCODER related code ################\n\n\nHIFIGAN_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`SeamlessM4TConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n# Copied from transformers.models.speecht5.modeling_speecht5.HifiGanResidualBlock\nclass HifiGanResidualBlock(nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5), leaky_relu_slope=0.1):\n        super().__init__()\n        self.leaky_relu_slope = leaky_relu_slope\n\n        self.convs1 = nn.ModuleList(\n            [\n                nn.Conv1d(\n                    channels,\n                    channels,\n                    kernel_size,\n                    stride=1,\n                    dilation=dilation[i],\n                    padding=self.get_padding(kernel_size, dilation[i]),\n                )\n                for i in range(len(dilation))\n            ]\n        )\n        self.convs2 = nn.ModuleList(\n            [\n                nn.Conv1d(\n                    channels,\n                    channels,\n                    kernel_size,\n                    stride=1,\n                    dilation=1,\n                    padding=self.get_padding(kernel_size, 1),\n                )\n                for _ in range(len(dilation))\n            ]\n        )\n\n    def get_padding(self, kernel_size, dilation=1):\n        return (kernel_size * dilation - dilation) // 2\n\n    def apply_weight_norm(self):\n        for layer in self.convs1:\n            nn.utils.weight_norm(layer)\n        for layer in self.convs2:\n            nn.utils.weight_norm(layer)\n\n    def remove_weight_norm(self):\n        for layer in self.convs1:\n            nn.utils.remove_weight_norm(layer)\n        for layer in self.convs2:\n            nn.utils.remove_weight_norm(layer)\n\n    def forward(self, hidden_states):\n        for conv1, conv2 in zip(self.convs1, self.convs2):\n            residual = hidden_states\n            hidden_states = nn.functional.leaky_relu(hidden_states, self.leaky_relu_slope)\n            hidden_states = conv1(hidden_states)\n            hidden_states = nn.functional.leaky_relu(hidden_states, self.leaky_relu_slope)\n            hidden_states = conv2(hidden_states)\n            hidden_states = hidden_states + residual\n        return hidden_states\n\n\nclass SeamlessM4TVariancePredictor(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n\n        embed_dim = config.unit_embed_dim\n        kernel_size = config.variance_predictor_kernel_size\n        var_pred_dropout = config.var_pred_dropout\n\n        self.conv1 = nn.Conv1d(\n            embed_dim,\n            embed_dim,\n            kernel_size=kernel_size,\n            padding=(kernel_size - 1) // 2,\n        )\n        self.activation_fuction = nn.ReLU()\n        self.ln1 = nn.LayerNorm(embed_dim)\n        self.dropout_module = nn.Dropout(p=var_pred_dropout)\n        self.conv2 = nn.Conv1d(\n            embed_dim,\n            embed_dim,\n            kernel_size=kernel_size,\n            padding=1,\n        )\n        self.ln2 = nn.LayerNorm(embed_dim)\n        self.proj = nn.Linear(embed_dim, 1)\n\n    def forward(self, hidden_states: Tensor) -> Tensor:\n        # Input: B x T x C; Output: B x T\n        hidden_states = self.conv1(hidden_states.transpose(1, 2))\n        hidden_states = self.activation_fuction(hidden_states).transpose(1, 2)\n        hidden_states = self.dropout_module(self.ln1(hidden_states))\n        hidden_states = self.conv2(hidden_states.transpose(1, 2))\n        hidden_states = self.activation_fuction(hidden_states).transpose(1, 2)\n        hidden_states = self.dropout_module(self.ln2(hidden_states))\n        return self.proj(hidden_states).squeeze(dim=2)\n\n\nclass SeamlessM4THifiGan(nn.Module):\n    def __init__(self, config: SeamlessM4TConfig):\n        super().__init__()\n        model_in_dim = config.unit_embed_dim + config.lang_embed_dim + config.spkr_embed_dim\n        self.leaky_relu_slope = config.leaky_relu_slope\n        self.num_kernels = len(config.resblock_kernel_sizes)\n        self.num_upsamples = len(config.upsample_rates)\n        self.conv_pre = nn.Conv1d(\n            model_in_dim,\n            config.upsample_initial_channel,\n            kernel_size=7,\n            stride=1,\n            padding=3,\n        )\n\n        self.upsampler = nn.ModuleList()\n        for i, (upsample_rate, kernel_size) in enumerate(zip(config.upsample_rates, config.upsample_kernel_sizes)):\n            self.upsampler.append(\n                nn.ConvTranspose1d(\n                    config.upsample_initial_channel // (2**i),\n                    config.upsample_initial_channel // (2 ** (i + 1)),\n                    kernel_size=kernel_size,\n                    stride=upsample_rate,\n                    padding=(kernel_size - upsample_rate) // 2,\n                )\n            )\n\n        self.resblocks = nn.ModuleList()\n        for i in range(len(self.upsampler)):\n            channels = config.upsample_initial_channel // (2 ** (i + 1))\n            for kernel_size, dilation in zip(config.resblock_kernel_sizes, config.resblock_dilation_sizes):\n                self.resblocks.append(HifiGanResidualBlock(channels, kernel_size, dilation, config.leaky_relu_slope))\n\n        self.conv_post = nn.Conv1d(channels, 1, kernel_size=7, stride=1, padding=3)\n\n    def forward(self, input_embeds: torch.FloatTensor) -> torch.FloatTensor:\n        r\"\"\"\n        Converts a log-mel spectrogram into a speech waveform. Passing a batch of log-mel spectrograms returns a batch\n        of speech waveforms. Passing a single, un-batched log-mel spectrogram returns a single, un-batched speech\n        waveform.\n\n        Args:\n            spectrogram (`torch.FloatTensor`):\n                Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,\n                model_in_dim)`, or un-batched and of shape `(sequence_length, model_in_dim)`. Note that `model_in_dim`\n                is the sum of `config.unit_embed_dim`, `config.lang_embed_dim` and `config.spkr_embed_dim`.\n\n        Returns:\n            `torch.FloatTensor`: Tensor containing the speech waveform. If the input spectrogram is batched, will be of\n            shape `(batch_size, num_frames,)`. If un-batched, will be of shape `(num_frames,)`.\n        \"\"\"\n\n        hidden_states = self.conv_pre(input_embeds)\n        for i in range(self.num_upsamples):\n            hidden_states = nn.functional.leaky_relu(hidden_states, self.leaky_relu_slope)\n            hidden_states = self.upsampler[i](hidden_states)\n\n            res_state = self.resblocks[i * self.num_kernels](hidden_states)\n            for j in range(1, self.num_kernels):\n                res_state += self.resblocks[i * self.num_kernels + j](hidden_states)\n            hidden_states = res_state / self.num_kernels\n\n        hidden_states = nn.functional.leaky_relu(hidden_states)\n        hidden_states = self.conv_post(hidden_states)\n        hidden_states = torch.tanh(hidden_states)\n\n        # remove seq-len dim since this collapses to 1\n        waveform = hidden_states.squeeze(1)\n\n        return waveform\n\n\n@add_start_docstrings(\n    \"\"\"Code HiFi-GAN vocoder as described in this [repository](https://github.com/facebookresearch/speech-resynthesis).\"\"\",\n    HIFIGAN_START_DOCSTRING,\n)\nclass SeamlessM4TCodeHifiGan(PreTrainedModel):\n    config_class = SeamlessM4TConfig\n    main_input_name = \"input_embeds\"\n    _no_split_modules = []\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.pad_token_id = config.t2u_pad_token_id\n        self.dur_predictor = SeamlessM4TVariancePredictor(config)\n\n        self.unit_embedding = nn.Embedding(config.unit_hifi_gan_vocab_size, config.unit_embed_dim)\n        self.speaker_embedding = nn.Embedding(config.vocoder_num_spkrs, config.spkr_embed_dim)\n        self.language_embedding = nn.Embedding(config.vocoder_num_langs, config.lang_embed_dim)\n\n        self.hifi_gan = SeamlessM4THifiGan(config)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def _get_dur_output_lengths(self, input_ids, dur_out):\n        \"\"\"\n        Computes the output length after the duration layer.\n        \"\"\"\n        unit_lengths = (input_ids != self.pad_token_id).sum(1)\n\n        # take care of edge cases where no padding or too many padding\n        unit_lengths = torch.clamp(unit_lengths, 0, dur_out.shape[1] - 1)\n\n        cumulative_dur_out = torch.cumsum(dur_out, dim=1)\n        unit_lengths = cumulative_dur_out.gather(dim=1, index=unit_lengths.unsqueeze(1)).squeeze()\n\n        return unit_lengths\n\n    def _get_output_hifigan_lengths(self, input_lengths: Union[torch.LongTensor, int]):\n        \"\"\"\n        Computes the output length of the hifigan convolutional layers\n        \"\"\"\n\n        def _conv_out_length(input_length, kernel_size, stride, pad, dilation=1):\n            # 1D convolutional layer output length formula taken\n            # from https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n            return (\n                torch.div(input_length + 2 * pad - dilation * (kernel_size - 1) - 1, stride, rounding_mode=\"floor\") + 1\n            )\n\n        def _transpose_conv_out_length(input_length, kernel_size, stride, pad, dilation=1):\n            return (input_length - 1) * stride - 2 * pad + dilation * (kernel_size - 1) + 1\n\n        # conv_pre\n        input_lengths = _conv_out_length(input_lengths, 7, 1, 3)\n\n        # upsampler\n        for i, (upsample_rate, kernel_size) in enumerate(\n            zip(self.config.upsample_rates, self.config.upsample_kernel_sizes)\n        ):\n            input_lengths = _transpose_conv_out_length(\n                input_lengths, kernel_size, upsample_rate, (kernel_size - upsample_rate) // 2\n            )\n\n        # resblock\n        for i in range(len(self.config.upsample_rates)):\n            for kernel_size, dilation in zip(self.config.resblock_kernel_sizes, self.config.resblock_dilation_sizes):\n                for dil in dilation:\n                    input_lengths = _conv_out_length(\n                        input_lengths, kernel_size, 1, (kernel_size - 1) * dil // 2, dilation=dil\n                    )\n\n                for dil in dilation:\n                    input_lengths = _conv_out_length(input_lengths, kernel_size, 1, (kernel_size - 1) // 2, dilation=1)\n\n        # conv_post\n        input_lengths = _conv_out_length(input_lengths, 7, 1, 3)\n\n        return input_lengths\n\n    def forward(\n        self, input_ids: torch.LongTensor, spkr_id: torch.Tensor, lang_id: torch.Tensor\n    ) -> Tuple[torch.Tensor]:\n        \"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary.\n\n                Indices can be obtained using [`SeamlessM4TTextToUnitForConditionalGeneration`]. [What are input\n                IDs?](../glossary#input-ids)\n            spkr_id (`int`, *optional*):\n                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.\n            tgt_lang (`str`, *optional*):\n                The language id to use as target language for translation.\n        \"\"\"\n        hidden_states = self.unit_embedding(input_ids).transpose(1, 2)\n        spkr = self.speaker_embedding(spkr_id).transpose(1, 2)\n        lang = self.language_embedding(lang_id).transpose(1, 2)\n\n        log_dur_pred = self.dur_predictor(hidden_states.transpose(1, 2))\n        dur_out = torch.clamp(torch.round((torch.exp(log_dur_pred) - 1)).long(), min=1)\n        # B x C x T\n        if hidden_states.size(0) == 1:\n            hidden_states = torch.repeat_interleave(hidden_states, dur_out.view(-1), dim=2)\n        else:\n            # if batched sample, need to interleave per sample, and pad -> loss of parallelism\n            if hidden_states.shape[0] > 1 and self.training:\n                logger.warning(\n                    \"\"\"`self.training=True` and you use batching. You lose parallelism during the hifigan\n                               forward pass because the samples are interleaved.\"\"\"\n                )\n            hidden_states = [\n                torch.repeat_interleave(hidden_state, duration, dim=-1).transpose(0, 1)\n                for (hidden_state, duration) in zip(hidden_states, dur_out)\n            ]\n\n            hidden_states = nn.utils.rnn.pad_sequence(hidden_states, batch_first=True).transpose(1, 2)\n\n        spkr = spkr.repeat(1, 1, hidden_states.shape[-1])\n        lang = lang.repeat(1, 1, hidden_states.shape[-1])\n        hidden_states = torch.cat([lang, hidden_states, spkr], dim=1)\n\n        hidden_states = self.hifi_gan(hidden_states)\n\n        unit_lengths = self._get_dur_output_lengths(input_ids, dur_out)\n        lengths = self._get_output_hifigan_lengths(unit_lengths)\n\n        return hidden_states, lengths\n\n    def _init_weights(self, module):\n        \"\"\"Initialize the weights.\"\"\"\n        if isinstance(module, (nn.Linear, nn.Conv1d, nn.ConvTranspose1d)):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def apply_weight_norm(self):\n        nn.utils.weight_norm(self.hifi_gan.conv_pre)\n        for layer in self.hifi_gan.upsampler:\n            nn.utils.weight_norm(layer)\n        for layer in self.hifi_gan.resblocks:\n            layer.apply_weight_norm()\n        nn.utils.weight_norm(self.hifi_gan.conv_post)\n\n    def remove_weight_norm(self):\n        nn.utils.remove_weight_norm(self.hifi_gan.conv_pre)\n        for layer in self.hifi_gan.upsampler:\n            nn.utils.remove_weight_norm(layer)\n        for layer in self.hifi_gan.resblocks:\n            layer.remove_weight_norm()\n        nn.utils.remove_weight_norm(self.hifi_gan.conv_post)\n\n\n############ WHOLE MODEL related code ################\n\n\n@add_start_docstrings(\n    \"The text-to-text SeamlessM4T Model transformer which can be used for T2TT.\",\n    SEAMLESS_M4T_START_DOCSTRING,\n)\nclass SeamlessM4TForTextToText(SeamlessM4TPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\"speech_encoder\", \"t2u_model\", \"vocoder\"]\n    main_input_name = \"input_ids\"\n\n    _tied_weights_keys = [\n        \"lm_head.weight\",\n        \"text_encoder.embed_tokens.weight\",\n        \"text_decoder.embed_tokens.weight\",\n    ]\n\n    def __init__(self, config: SeamlessM4TConfig):\n        super().__init__(config)\n\n        self.shared = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n\n        self.text_encoder = SeamlessM4TEncoder(config, self.shared)\n        self.text_decoder = SeamlessM4TDecoder(config, self.shared)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_encoder(self):\n        return self.text_encoder\n\n    def get_decoder(self):\n        return self.text_decoder\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_input_embeddings(self):\n        return self.text_decoder.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.text_encoder.embed_tokens = value\n        self.text_decoder.embed_tokens = value\n        self.shared = value\n\n    def _tie_weights(self):\n        if self.config.tie_word_embeddings:\n            self._tie_or_clone_weights(self.text_encoder.embed_tokens, self.shared)\n            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n            self._tie_or_clone_weights(self.lm_head, self.shared)\n\n    @add_start_docstrings_to_model_forward(M4T_TEXT_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ) -> Union[Seq2SeqLMOutput, Tuple[torch.FloatTensor]]:\n        if labels is not None:\n            if use_cache:\n                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n            use_cache = False\n            if decoder_input_ids is None and decoder_inputs_embeds is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if encoder_outputs is None:\n            encoder_outputs = self.text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        encoder_attention_mask = attention_mask\n\n        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n        decoder_outputs = self.text_decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        lm_logits = self.lm_head(decoder_outputs[0])\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            labels = labels.to(lm_logits.device)\n            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            outputs = decoder_outputs + encoder_outputs\n            output = (lm_logits,) + outputs[1:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,\n            logits=lm_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    def generate(\n        self,\n        input_ids=None,\n        tgt_lang=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Generates sequences of token ids.\n\n        <Tip warning={true}>\n\n        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n        model's default generation configuration. You can override any `generation_config` by passing the corresponding\n        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n\n        For an overview of generation strategies and code examples, check out the [following\n        guide](./generation_strategies).\n\n        </Tip>\n\n        Parameters:\n            input_ids (`torch.Tensor` of varying shape depending on the modality, *optional*):\n                Indices of input sequence tokens in the vocabulary.\n\n                Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See\n                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            tgt_lang (`str`, *optional*):\n                The language to use as target language for translation.\n            generation_config (`~generation.GenerationConfig`, *optional*):\n                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n                passed to generate matching the attributes of `generation_config` will override them. If\n                `generation_config` is not provided, the default will be used, which had the following loading\n                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n                default values, whose documentation should be checked to parameterize generation.\n            logits_processor (`LogitsProcessorList`, *optional*):\n                Custom logits processors that complement the default logits processors built from arguments and\n                generation config. If a logit processor is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            stopping_criteria (`StoppingCriteriaList`, *optional*):\n                Custom stopping criteria that complement the default stopping criteria built from arguments and a\n                generation config. If a stopping criteria is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                Retrieval](https://arxiv.org/abs/2010.00904).\n            synced_gpus (`bool`, *optional*, defaults to `False`):\n                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n            kwargs (`Dict[str, Any]`, *optional*):\n                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                forwarded to the `forward` function of the model.\n\n        Return:\n            [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n            or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`. The possible\n            [`~utils.ModelOutput`] types are:\n                - [`~generation.GenerateEncoderDecoderOutput`],\n                - [`~generation.GenerateBeamEncoderDecoderOutput`]\n        \"\"\"\n        # prepare text_decoder_input_ids\n        text_decoder_input_ids = kwargs.pop(\"decoder_input_ids\", None)\n        # overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.\n        if tgt_lang is not None:\n            batch_size = len(input_ids) if input_ids is not None else len(kwargs.get(\"inputs_embeds\"))\n\n            if hasattr(self.generation_config, \"text_decoder_lang_to_code_id\"):\n                # also accept __xxx__\n                tgt_lang = tgt_lang.replace(\"__\", \"\")\n                if tgt_lang not in self.generation_config.text_decoder_lang_to_code_id:\n                    raise ValueError(\n                        f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model. Please specify a `tgt_lang` in\n                        {', '.join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n                    )\n                # tgt_lang gets priority over decoder input ids\n                text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n            else:\n                raise ValueError(\n                    \"\"\"This model generation config doesn't have a `text_decoder_lang_to_code_id` key which maps\n                    the target language to the right token id. Make sure to load the right generation config.\"\"\"\n                )\n        else:\n            # only a warning, otherwise errors appear in the tests\n            logger.warning(\n                \"\"\"You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get\n                a correct generation, otherwise the generation will probably make no sense.\"\"\"\n            )\n\n        return super().generate(\n            input_ids,\n            generation_config,\n            logits_processor,\n            stopping_criteria,\n            prefix_allowed_tokens_fn,\n            synced_gpus,\n            decoder_input_ids=text_decoder_input_ids,\n            **kwargs,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # cut decoder_input_ids if past is used\n        if past_key_values is not None:\n            decoder_input_ids = decoder_input_ids[:, -1:]\n\n        return {\n            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past_key_values,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"use_cache\": use_cache,\n        }\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            # cached cross_attention states don't have to be reordered -> they are always the same\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"The speech-to-text SeamlessM4T Model transformer which can be used for S2TT.\",\n    SEAMLESS_M4T_START_DOCSTRING,\n)\nclass SeamlessM4TForSpeechToText(SeamlessM4TPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\"text_decoder\", \"t2u_model\", \"vocoder\"]\n    main_input_name = \"input_features\"\n\n    _tied_weights_keys = [\n        \"lm_head.weight\",\n        \"text_decoder.embed_tokens.weight\",\n    ]\n\n    def __init__(self, config: SeamlessM4TConfig):\n        super().__init__(config)\n\n        self.shared = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n        self.speech_encoder = SeamlessM4TSpeechEncoder(config)\n        self.text_decoder = SeamlessM4TDecoder(config, self.shared)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_encoder(self):\n        return self.speech_encoder\n\n    def get_decoder(self):\n        return self.text_decoder\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_input_embeddings(self):\n        return self.text_decoder.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.text_decoder.embed_tokens = value\n\n    def _tie_weights(self):\n        if self.config.tie_word_embeddings:\n            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n            self._tie_or_clone_weights(self.lm_head, self.shared)\n\n    @add_start_docstrings_to_model_forward(M4T_SPEECH_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_features: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ) -> Union[Seq2SeqLMOutput, Tuple[torch.FloatTensor]]:\n        if labels is not None:\n            if use_cache:\n                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n            use_cache = False\n            if decoder_input_ids is None and decoder_inputs_embeds is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if encoder_outputs is None:\n            encoder_outputs = self.speech_encoder(\n                input_features=input_features,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        encoder_attention_mask = attention_mask\n        if attention_mask is not None:\n            sub_sampled_lengths = self._compute_sub_sample_lengths_from_attention_mask(attention_mask).to(\n                encoder_outputs[0].device\n            )\n            encoder_attention_mask = _compute_new_attention_mask(\n                hidden_states=encoder_outputs[0], seq_lens=sub_sampled_lengths\n            )\n\n        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n        decoder_outputs = self.text_decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        lm_logits = self.lm_head(decoder_outputs[0])\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            labels = labels.to(lm_logits.device)\n            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            outputs = decoder_outputs + encoder_outputs\n            output = (lm_logits,) + outputs[1:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,\n            logits=lm_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    def generate(\n        self,\n        input_features=None,\n        tgt_lang=None,\n        generation_config=None,\n        logits_processor=None,\n        stopping_criteria=None,\n        prefix_allowed_tokens_fn=None,\n        synced_gpus=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Generates sequences of token ids.\n\n        <Tip warning={true}>\n\n        Most generation-controlling parameters are set in `generation_config` which, if not passed, will be set to the\n        model's default generation configuration. You can override any `generation_config` by passing the corresponding\n        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n\n        For an overview of generation strategies and code examples, check out the [following\n        guide](./generation_strategies).\n\n        </Tip>\n\n        Parameters:\n            input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n                [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n\n            tgt_lang (`str`, *optional*):\n                The language to use as target language for translation.\n            generation_config (`~generation.GenerationConfig`, *optional*):\n                The generation configuration to be used as base parametrization for the generation call. `**kwargs`\n                passed to generate matching the attributes of `generation_config` will override them. If\n                `generation_config` is not provided, the default will be used, which had the following loading\n                priority: 1) from the `generation_config.json` model file, if it exists; 2) from the model\n                configuration. Please note that unspecified parameters will inherit [`~generation.GenerationConfig`]'s\n                default values, whose documentation should be checked to parameterize generation.\n            logits_processor (`LogitsProcessorList`, *optional*):\n                Custom logits processors that complement the default logits processors built from arguments and\n                generation config. If a logit processor is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            stopping_criteria (`StoppingCriteriaList`, *optional*):\n                Custom stopping criteria that complement the default stopping criteria built from arguments and a\n                generation config. If a stopping criteria is passed that is already created with the arguments or a\n                generation config an error is thrown. This feature is intended for advanced users.\n            prefix_allowed_tokens_fn (`Callable[[int, torch.Tensor], List[int]]`, *optional*):\n                If provided, this function constraints the beam search to allowed tokens only at each step. If not\n                provided no constraint is applied. This function takes 2 arguments: the batch ID `batch_id` and\n                `input_ids`. It has to return a list with the allowed tokens for the next generation step conditioned\n                on the batch ID `batch_id` and the previously generated tokens `inputs_ids`. This argument is useful\n                for constrained generation conditioned on the prefix, as described in [Autoregressive Entity\n                Retrieval](https://arxiv.org/abs/2010.00904).\n            synced_gpus (`bool`, *optional*, defaults to `False`):\n                Whether to continue running the while loop until max_length (needed for ZeRO stage 3)\n            kwargs (`Dict[str, Any]`, *optional*):\n                Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                forwarded to the `forward` function of the model.\n\n        Return:\n            [`~utils.ModelOutput`] or `torch.LongTensor`: A [`~utils.ModelOutput`] (if `return_dict_in_generate=True`\n            or when `config.return_dict_in_generate=True`) or a `torch.FloatTensor`. The possible\n            [`~utils.ModelOutput`] types are:\n                - [`~generation.GenerateEncoderDecoderOutput`],\n                - [`~generation.GenerateBeamEncoderDecoderOutput`]\n        \"\"\"\n        text_decoder_input_ids = kwargs.pop(\"decoder_input_ids\", None)\n        # overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.\n        if tgt_lang is not None:\n            inputs = kwargs.get(\"input_embeds\") if input_features is None else input_features\n            inputs = (\n                inputs\n                if inputs is not None\n                else kwargs.get(\"encoder_outputs\", {\"last_hidden_state\": None})[\"last_hidden_state\"]\n            )\n            batch_size = len(inputs)\n\n            if hasattr(self.generation_config, \"text_decoder_lang_to_code_id\"):\n                # also accept __xxx__\n                tgt_lang = tgt_lang.replace(\"__\", \"\")\n                if tgt_lang not in self.generation_config.text_decoder_lang_to_code_id:\n                    raise ValueError(\n                        f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model. Please specify a `tgt_lang` in\n                        {', '.join(self.generation_config.text_decoder_lang_to_code_id.keys())}\"\"\"\n                    )\n                # tgt_lang gets priority over decoder input ids\n                text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n            else:\n                raise ValueError(\n                    \"\"\"This model generation config doesn't have a `text_decoder_lang_to_code_id` key which maps\n                    the target language to the right token id. Make sure to load the right generation config.\"\"\"\n                )\n        else:\n            # only a warning, otherwise errors appear in the tests\n            logger.warning(\n                \"\"\"You must either specify a `tgt_lang` or pass a correct `text_decoder_input_ids` to get\n                a correct generation, otherwise the generation will probably make no sense.\"\"\"\n            )\n        return super().generate(\n            input_features,\n            generation_config,\n            logits_processor,\n            stopping_criteria,\n            prefix_allowed_tokens_fn,\n            synced_gpus,\n            decoder_input_ids=text_decoder_input_ids,\n            **kwargs,\n        )\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # cut decoder_input_ids if past is used\n        if past_key_values is not None:\n            decoder_input_ids = decoder_input_ids[:, -1:]\n\n        return {\n            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past_key_values,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"use_cache\": use_cache,\n        }\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            # cached cross_attention states don't have to be reordered -> they are always the same\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"The text-to-speech SeamlessM4T Model transformer which can be used for T2ST.\",\n    SEAMLESS_M4T_START_DOCSTRING,\n)\nclass SeamlessM4TForTextToSpeech(SeamlessM4TPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\"speech_encoder\"]\n    main_input_name = \"input_ids\"\n\n    _tied_weights_keys = [\n        \"lm_head.weight\",\n        \"text_encoder.embed_tokens.weight\",\n        \"text_decoder.embed_tokens.weight\",\n    ]\n\n    def __init__(self, config: SeamlessM4TConfig):\n        super().__init__(config)\n\n        self.shared = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n\n        self.text_encoder = SeamlessM4TEncoder(config, self.shared)\n        self.text_decoder = SeamlessM4TDecoder(config, self.shared)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        self.t2u_model = SeamlessM4TTextToUnitForConditionalGeneration(config)\n        self.vocoder = SeamlessM4TCodeHifiGan(config)\n\n    def get_encoder(self):\n        return self.text_encoder\n\n    def get_decoder(self):\n        return self.text_decoder\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_input_embeddings(self):\n        return self.text_decoder.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.text_encoder.embed_tokens = value\n        self.text_decoder.embed_tokens = value\n        self.shared = value\n\n    def _tie_weights(self):\n        if self.config.tie_word_embeddings:\n            self._tie_or_clone_weights(self.text_encoder.embed_tokens, self.shared)\n            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n            self._tie_or_clone_weights(self.lm_head, self.shared)\n\n    @add_start_docstrings_to_model_forward(M4T_TEXT_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Seq2SeqLMOutput, Tuple[torch.FloatTensor]]:\n        if labels is not None:\n            if use_cache:\n                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n            use_cache = False\n            if decoder_input_ids is None and decoder_inputs_embeds is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if encoder_outputs is None:\n            # if encoder_outputs is not None, it's probably used within a .generate method so no need to warn\n            logger.warning(\n                \"This is the same forward method as `SeamlessM4TForTextToText`.\"\n                \"It doesn't use the text-to-unit model `SeamlessM4TTextToUnitForConditionalGeneration`.\"\n                \"If you want to generate speech, use the `.generate` method.\"\n            )\n            encoder_outputs = self.text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        encoder_attention_mask = attention_mask\n\n        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n        decoder_outputs = self.text_decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        lm_logits = self.lm_head(decoder_outputs[0])\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            labels = labels.to(lm_logits.device)\n            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            outputs = decoder_outputs + encoder_outputs\n            output = (lm_logits,) + outputs[1:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,\n            logits=lm_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        return_intermediate_token_ids: Optional[bool] = None,\n        tgt_lang: Optional[str] = None,\n        spkr_id: Optional[int] = 0,\n        **kwargs,\n    ) -> Union[torch.Tensor, SeamlessM4TGenerationOutput]:\n        \"\"\"\n        Generates translated audio waveforms.\n\n        <Tip>\n\n        This method successively calls the `.generate` function of two different sub-models. You can specify keyword\n        arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments\n        that will be passed to one of them.\n\n        For example, calling `.generate(input_ids, num_beams=4, speech_do_sample=True)` will successively perform\n        beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.\n\n        For an overview of generation strategies and code examples, check out the [following\n        guide](./generation_strategies).\n\n        </Tip>\n\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary.\n\n                Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See\n                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            return_intermediate_token_ids (`bool`, *optional*):\n                If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want\n                to get translated text alongside the audio.\n            tgt_lang (`str`, *optional*):\n                The language to use as target language for translation.\n            spkr_id (`int`, *optional*, defaults to 0):\n                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.\n            kwargs (*optional*):\n                Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword\n                arguments are of two types:\n\n                    - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,\n                    except for `decoder_input_ids` which will only be passed through the text components.\n                    - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the\n                    text model and speech model respectively. It has the priority over the keywords without a prefix.\n\n                    This means you can, for example, specify a generation strategy for one generation but not for the\n                    other.\n\n\n        Returns:\n            `Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`:\n            - If `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].\n            - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,\n              sequence_length)`and and `waveform_lengths` which gives the length of each sample.\n        \"\"\"\n        batch_size = len(input_ids) if input_ids is not None else len(kwargs.get(\"inputs_embeds\"))\n\n        if tgt_lang is None:\n            raise ValueError(\"You must specify a `tgt_lang` to generate translated speech.\")\n        else:\n            # also accept __xxx__\n            tgt_lang = tgt_lang.replace(\"__\", \"\")\n            for key in [\"text_decoder_lang_to_code_id\", \"t2u_lang_code_to_id\", \"vocoder_lang_code_to_id\"]:\n                lang_code_to_id = getattr(self.generation_config, key, None)\n                if lang_code_to_id is None:\n                    raise ValueError(\n                        f\"\"\"This model generation config doesn't have a `{key}` key which maps the target language\n                        to the right token id. Make sure to load the right generation config.\"\"\"\n                    )\n                elif tgt_lang not in lang_code_to_id:\n                    raise ValueError(\n                        f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model.\n                    Please specify a `tgt_lang` in {','.join(lang_code_to_id.keys())}. Note that SeamlessM4T supports\n                    more languages for text translation than for speech synthesis.\"\"\"\n                    )\n\n        kwargs_text, kwargs_speech = format_speech_generation_kwargs(kwargs)\n        kwargs_text[\"output_hidden_states\"] = True\n        kwargs_text[\"return_dict_in_generate\"] = True\n        kwargs_text[\"output_scores\"] = True\n\n        text_decoder_input_ids = kwargs_text.get(\"decoder_input_ids\")\n\n        # overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.\n        text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n\n        kwargs_text[\"decoder_input_ids\"] = text_decoder_input_ids\n\n        # first generation\n        text_generation_output = super().generate(input_ids, **kwargs_text)\n        sequences = text_generation_output.sequences\n\n        # prepare second generation\n        num_return_sequences = len(sequences) // batch_size\n        attention_mask = kwargs_speech.get(\"attention_mask\", kwargs_text.get(\"attention_mask\", None))\n\n        encoder_hidden_states = text_generation_output.encoder_hidden_states[-1]\n\n        # take care of num_return_sequences\n        # take most probable hidden states per batch of return_sequences\n        # (batch_size*num_return_sequences, ...) -> (batch_size,...)\n        if num_return_sequences > 1:\n            idx_most_probable_sequences_per_batch = text_generation_output.sequences_scores.view(batch_size, -1)\n            idx_most_probable_sequences_per_batch = idx_most_probable_sequences_per_batch.argmax(-1)\n            idx_most_probable_sequences_per_batch = (\n                idx_most_probable_sequences_per_batch + torch.arange(batch_size).to(self.device) * num_return_sequences\n            )\n            sequences = sequences[idx_most_probable_sequences_per_batch]\n\n        # get decoder last hidden state - must do a pass through the text decoder\n        t2u_input_embeds = self.text_decoder(\n            input_ids=sequences,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=attention_mask,\n        ).last_hidden_state\n\n        pad_token_id = self.generation_config.pad_token_id\n\n        # Compute new attention mask\n        seq_lens = (sequences != pad_token_id).int().sum(1)\n        t2u_model_attention_mask = _compute_new_attention_mask(t2u_input_embeds, seq_lens)\n        kwargs_speech[\"attention_mask\"] = t2u_model_attention_mask\n\n        # Compute t2u decoder_input_ids\n        t2u_decoder_input_ids = kwargs_speech.get(\"decoder_input_ids\")\n        t2u_tgt_lang_id = self.generation_config.t2u_lang_code_to_id.get(tgt_lang)\n        t2u_decoder_input_ids = torch.tensor([[self.config.t2u_eos_token_id, t2u_tgt_lang_id]] * batch_size).to(\n            self.device\n        )\n        kwargs_speech[\"decoder_input_ids\"] = t2u_decoder_input_ids\n\n        # second generation\n        unit_ids = self.t2u_model.generate(inputs_embeds=t2u_input_embeds, **kwargs_speech)\n        output_unit_ids = unit_ids.detach().clone()\n\n        # get rid of t2u_decoder_input_ids\n        unit_ids = unit_ids[:, kwargs_speech[\"decoder_input_ids\"].shape[1] :]\n        # replace eos per pad\n        unit_ids[unit_ids == self.config.t2u_eos_token_id] = self.config.t2u_pad_token_id\n        # offset of control symbols\n        unit_ids = torch.where(\n            unit_ids == self.config.t2u_pad_token_id, unit_ids, unit_ids - self.config.vocoder_offset\n        )\n\n        vocoder_tgt_lang_id = self.generation_config.vocoder_lang_code_to_id.get(tgt_lang)\n        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids)).to(self.device)\n\n        spkr_id = torch.tensor([[spkr_id]] * len(unit_ids)).to(self.device)\n\n        waveform, waveform_lengths = self.vocoder(input_ids=unit_ids, spkr_id=spkr_id, lang_id=vocoder_tgt_lang_id)\n\n        if return_intermediate_token_ids:\n            return SeamlessM4TGenerationOutput(\n                waveform=waveform,\n                waveform_lengths=waveform_lengths,\n                sequences=sequences,\n                unit_sequences=output_unit_ids,\n            )\n\n        return waveform, waveform_lengths\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # cut decoder_input_ids if past is used\n        if past_key_values is not None:\n            decoder_input_ids = decoder_input_ids[:, -1:]\n\n        return {\n            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past_key_values,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"use_cache\": use_cache,\n        }\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            # cached cross_attention states don't have to be reordered -> they are always the same\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n            )\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"The speech-to-speech SeamlessM4T Model transformer which can be used for S2ST.\",\n    SEAMLESS_M4T_START_DOCSTRING,\n)\nclass SeamlessM4TForSpeechToSpeech(SeamlessM4TPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [\"text_encoder\"]\n    main_input_name = \"input_features\"\n\n    _tied_weights_keys = [\n        \"lm_head.weight\",\n        \"text_decoder.embed_tokens.weight\",\n    ]\n\n    def __init__(self, config):\n        super().__init__(config)\n\n        self.shared = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n        self.speech_encoder = SeamlessM4TSpeechEncoder(config)\n        self.text_decoder = SeamlessM4TDecoder(config, self.shared)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        self.t2u_model = SeamlessM4TTextToUnitForConditionalGeneration(config)\n        self.vocoder = SeamlessM4TCodeHifiGan(config)\n\n    def get_encoder(self):\n        return self.speech_encoder\n\n    def get_decoder(self):\n        return self.text_decoder\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_input_embeddings(self):\n        return self.text_decoder.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.text_decoder.embed_tokens = value\n\n    def _tie_weights(self):\n        if self.config.tie_word_embeddings:\n            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n            self._tie_or_clone_weights(self.lm_head, self.shared)\n\n    @add_start_docstrings_to_model_forward(M4T_SPEECH_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_features: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ) -> Union[Seq2SeqLMOutput, Tuple[torch.FloatTensor]]:\n        if labels is not None:\n            if use_cache:\n                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n            use_cache = False\n            if decoder_input_ids is None and decoder_inputs_embeds is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if encoder_outputs is None:\n            # if encoder_outputs is not None, it's probably used within a .generate method so no need to warn\n            logger.warning(\n                \"This is the same forward method as `SeamlessM4TForSpeechToText`. It doesn't use `self.t2u_model`.\"\n                \"If you want to generate speech, use the `generate` method.\"\n            )\n\n            encoder_outputs = self.speech_encoder(\n                input_features=input_features,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        encoder_attention_mask = attention_mask\n        if attention_mask is not None:\n            sub_sampled_lengths = self._compute_sub_sample_lengths_from_attention_mask(attention_mask).to(\n                encoder_outputs[0].device\n            )\n            encoder_attention_mask = _compute_new_attention_mask(\n                hidden_states=encoder_outputs[0], seq_lens=sub_sampled_lengths\n            )\n\n        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n        decoder_outputs = self.text_decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        lm_logits = self.lm_head(decoder_outputs[0])\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            labels = labels.to(lm_logits.device)\n            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            outputs = decoder_outputs + encoder_outputs\n            output = (lm_logits,) + outputs[1:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,\n            logits=lm_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        input_features: Optional[torch.Tensor] = None,\n        return_intermediate_token_ids: Optional[bool] = None,\n        tgt_lang: Optional[str] = None,\n        spkr_id: Optional[int] = 0,\n        **kwargs,\n    ) -> Union[torch.Tensor, SeamlessM4TGenerationOutput]:\n        \"\"\"\n        Generates translated audio waveforms.\n\n        <Tip>\n\n        This method successively calls the `.generate` function of two different sub-models. You can specify keyword\n        arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments\n        that will be passed to one of them.\n\n        For example, calling `.generate(input_features, num_beams=4, speech_do_sample=True)` will successively perform\n        beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.\n\n        For an overview of generation strategies and code examples, check out the [following\n        guide](./generation_strategies).\n\n        </Tip>\n\n        Args:\n            input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n                [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n            return_intermediate_token_ids (`bool`, *optional*):\n                If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want\n                to get translated text alongside the audio.\n            tgt_lang (`str`, *optional*):\n                The language to use as target language for translation.\n            spkr_id (`int`, *optional*, defaults to 0):\n                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.\n\n            kwargs (*optional*):\n                Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword\n                arguments are of two types:\n\n                    - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,\n                    except for `decoder_input_ids` which will only be passed through the text components.\n                    - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the\n                    text model and speech model respectively. It has the priority over the keywords without a prefix.\n\n                    This means you can, for example, specify a generation strategy for one generation but not for the\n                    other.\n\n\n        Returns:\n            `Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`:\n            - If `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].\n            - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,\n              sequence_length)`and and `waveform_lengths` which gives the length of each sample.\n        \"\"\"\n        batch_size = len(input_features) if input_features is not None else len(kwargs.get(\"inputs_embeds\"))\n\n        if tgt_lang is None:\n            raise ValueError(\"You must specify a `tgt_lang` to generate translated speech.\")\n        else:\n            # also accept __xxx__\n            tgt_lang = tgt_lang.replace(\"__\", \"\")\n            for key in [\"text_decoder_lang_to_code_id\", \"t2u_lang_code_to_id\", \"vocoder_lang_code_to_id\"]:\n                lang_code_to_id = getattr(self.generation_config, key, None)\n                if lang_code_to_id is None:\n                    raise ValueError(\n                        f\"\"\"This model generation config doesn't have a `{key}` key which maps the target language\n                        to the right token id. Make sure to load the right generation config.\"\"\"\n                    )\n                elif tgt_lang not in lang_code_to_id:\n                    raise ValueError(\n                        f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model.\n                    Please specify a `tgt_lang` in {','.join(lang_code_to_id.keys())}. Note that SeamlessM4T supports\n                    more languages for text translation than for speech synthesis.\"\"\"\n                    )\n\n        kwargs_text, kwargs_speech = format_speech_generation_kwargs(kwargs)\n        kwargs_text[\"output_hidden_states\"] = True\n        kwargs_text[\"return_dict_in_generate\"] = True\n        kwargs_text[\"output_scores\"] = True\n\n        text_decoder_input_ids = kwargs_text.get(\"decoder_input_ids\")\n        # overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.\n        text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n\n        kwargs_text[\"decoder_input_ids\"] = text_decoder_input_ids\n\n        # first generation\n        text_generation_output = super().generate(input_features, **kwargs_text)\n        sequences = text_generation_output.sequences\n\n        # prepare second generation\n        num_return_sequences = len(sequences) // batch_size\n        attention_mask = kwargs_speech.get(\"attention_mask\", kwargs_text.get(\"attention_mask\", None))\n\n        # get last_hidden_state from encoder\n        encoder_hidden_states = self.speech_encoder(input_features=input_features, attention_mask=attention_mask)[0]\n\n        # input modality = speech so new attention mask for the decoder\n        if attention_mask is not None:\n            sub_sampled_lengths = self._compute_sub_sample_lengths_from_attention_mask(attention_mask).to(\n                encoder_hidden_states.device\n            )\n            attention_mask = _compute_new_attention_mask(\n                hidden_states=encoder_hidden_states, seq_lens=sub_sampled_lengths\n            )\n\n        # take care of num_return_sequences\n        # take most probable hidden states per batch of return_sequences\n        # (batch_size*num_return_sequences, ...) -> (batch_size,...)\n        if num_return_sequences > 1:\n            idx_most_probable_sequences_per_batch = text_generation_output.sequences_scores.view(batch_size, -1)\n            idx_most_probable_sequences_per_batch = idx_most_probable_sequences_per_batch.argmax(-1)\n            idx_most_probable_sequences_per_batch = (\n                idx_most_probable_sequences_per_batch + torch.arange(batch_size).to(self.device) * num_return_sequences\n            )\n            sequences = sequences[idx_most_probable_sequences_per_batch]\n\n        # get decoder last hidden state - must do a pass through the text decoder\n        t2u_input_embeds = self.text_decoder(\n            input_ids=sequences,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=attention_mask,\n        ).last_hidden_state\n\n        pad_token_id = self.generation_config.pad_token_id\n\n        # Compute new attention mask\n        seq_lens = (sequences != pad_token_id).int().sum(1)\n        t2u_model_attention_mask = _compute_new_attention_mask(t2u_input_embeds, seq_lens)\n        kwargs_speech[\"attention_mask\"] = t2u_model_attention_mask\n\n        # Compute t2u decoder_input_ids\n        t2u_decoder_input_ids = kwargs_speech.get(\"decoder_input_ids\")\n        t2u_tgt_lang_id = self.generation_config.t2u_lang_code_to_id.get(tgt_lang)\n        t2u_decoder_input_ids = torch.tensor([[self.config.t2u_eos_token_id, t2u_tgt_lang_id]] * batch_size).to(\n            self.device\n        )\n        kwargs_speech[\"decoder_input_ids\"] = t2u_decoder_input_ids\n\n        # second generation\n        unit_ids = self.t2u_model.generate(inputs_embeds=t2u_input_embeds, **kwargs_speech)\n        output_unit_ids = unit_ids.detach().clone()\n\n        # get rid of t2u_decoder_input_ids\n        unit_ids = unit_ids[:, kwargs_speech[\"decoder_input_ids\"].shape[1] :]\n        # replace eos per pad\n        unit_ids[unit_ids == self.config.t2u_eos_token_id] = self.config.t2u_pad_token_id\n        # offset of control symbols\n        unit_ids = torch.where(\n            unit_ids == self.config.t2u_pad_token_id, unit_ids, unit_ids - self.config.vocoder_offset\n        )\n\n        vocoder_tgt_lang_id = self.generation_config.vocoder_lang_code_to_id.get(tgt_lang)\n        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids)).to(self.device)\n\n        spkr_id = torch.tensor([[spkr_id]] * len(unit_ids)).to(self.device)\n\n        waveform, waveform_lengths = self.vocoder(input_ids=unit_ids, spkr_id=spkr_id, lang_id=vocoder_tgt_lang_id)\n\n        if return_intermediate_token_ids:\n            return SeamlessM4TGenerationOutput(\n                waveform=waveform,\n                waveform_lengths=waveform_lengths,\n                sequences=sequences,\n                unit_sequences=output_unit_ids,\n            )\n\n        return waveform, waveform_lengths\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            # cached cross_attention states don't have to be reordered -> they are always the same\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n            )\n        return reordered_past\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # cut decoder_input_ids if past is used\n        if past_key_values is not None:\n            decoder_input_ids = decoder_input_ids[:, -1:]\n\n        return {\n            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past_key_values,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"use_cache\": use_cache,\n        }\n\n\n@add_start_docstrings(\n    \"The original SeamlessM4T Model transformer which can be used for every tasks available (S2ST, S2TT, T2TT, T2ST).\",\n    SEAMLESS_M4T_START_DOCSTRING,\n    \"\"\"\n        current_modality (`str`, *optional*, defaults to `\"text\"`):\n            Default modality. Used to initialize the model.\n    \"\"\",\n)\nclass SeamlessM4TModel(SeamlessM4TPreTrainedModel):\n    _tied_weights_keys = [\n        \"lm_head.weight\",\n        \"text_encoder.embed_tokens.weight\",\n        \"text_decoder.embed_tokens.weight\",\n    ]\n\n    def __init__(self, config, current_modality=\"text\"):\n        super().__init__(config)\n\n        self.shared = nn.Embedding(config.vocab_size, config.hidden_size, config.pad_token_id)\n\n        self.text_encoder = SeamlessM4TEncoder(config, self.shared)\n        self.speech_encoder = SeamlessM4TSpeechEncoder(config)\n        self.text_decoder = SeamlessM4TDecoder(config, self.shared)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n        self.current_modality = current_modality\n        if current_modality == \"speech\":\n            self.main_input_name = \"input_features\"\n\n        # these models already call post_init in their initialization\n        self.t2u_model = SeamlessM4TTextToUnitForConditionalGeneration(config)\n        self.vocoder = SeamlessM4TCodeHifiGan(config)\n\n    def set_modality(self, modality=\"text\"):\n        if modality == \"text\":\n            self.main_input_name = \"input_ids\"\n            self.current_modality = \"text\"\n        elif modality == \"speech\":\n            self.main_input_name = \"input_features\"\n            self.current_modality = \"speech\"\n        else:\n            raise ValueError(f\"`modality={modality}` is not a valid modality. It must be `text` or `speech`.\")\n\n    def get_encoder(self):\n        if self.current_modality == \"text\":\n            return self.text_encoder\n        else:\n            return self.speech_encoder\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def get_input_embeddings(self):\n        return self.text_decoder.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.text_encoder.embed_tokens = value\n        self.text_decoder.embed_tokens = value\n        self.shared = value\n\n    def _tie_weights(self):\n        if self.config.tie_word_embeddings:\n            self._tie_or_clone_weights(self.text_encoder.embed_tokens, self.shared)\n            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n            self._tie_or_clone_weights(self.lm_head, self.shared)\n\n    @add_start_docstrings_to_model_forward(M4T_MODEL_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: Optional[torch.LongTensor] = None,\n        input_features: Optional[torch.FloatTensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        decoder_input_ids: Optional[torch.LongTensor] = None,\n        decoder_attention_mask: Optional[torch.LongTensor] = None,\n        encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        **kwargs,\n    ) -> Union[Seq2SeqLMOutput, Tuple[torch.FloatTensor]]:\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        if labels is not None:\n            if use_cache:\n                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n            use_cache = False\n            if decoder_input_ids is None and decoder_inputs_embeds is None:\n                decoder_input_ids = shift_tokens_right(\n                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n                )\n\n        if input_ids is None and input_features is None and inputs_embeds is None and encoder_outputs is None:\n            raise ValueError(\n                \"`input_ids`,`input_features`, `inputs_embeds` and `encoder_outputs` are all empty. Make sure at least one of them is not.\"\n            )\n        elif input_features is not None:\n            if input_ids is not None:\n                logger.warning(\n                    \"`input_ids` is not `None` but `input_features` has been given.\"\n                    \"`input_features` will be used in priority through the `speech_encoder`. \"\n                    \"Make sure that `input_features` and `input_ids` are mutually exclusive.\"\n                )\n\n            if inputs_embeds is not None:\n                logger.warning(\n                    \"`inputs_embeds` is not `None` but `input_features` has been given.\"\n                    \"`input_features` will be used in priority through `speech_encoder`. \"\n                    \"`inputs_embeds` will be ignored.\"\n                )\n\n            # if encoder_outputs is not None, it's probably used within a .generate method so no need to warn\n            logger.warning(\n                \"This calls the same method `forward` as `SeamlessM4TForTextToText` and `SeamlessM4TForSpeechToText`\"\n                \"depending on the input modality. If you want to generate speech, use the `generate` method.\"\n            )\n\n            self.set_modality(\"speech\")\n\n            encoder_outputs = self.speech_encoder(\n                input_features=input_features,\n                attention_mask=attention_mask,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n\n        elif input_ids is not None or inputs_embeds is not None:\n            # if encoder_outputs is not None, it's probably used within a .generate method so no need to warn\n            logger.warning(\n                \"This calls the same method `forward` as `SeamlessM4TForTextToText` and `SeamlessM4TForSpeechToText`\"\n                \"depending on the input modality. If you want to generate speech, use the `generate` method.\"\n            )\n            self.set_modality(\"text\")\n            encoder_outputs = self.text_encoder(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                inputs_embeds=inputs_embeds,\n                output_attentions=output_attentions,\n                output_hidden_states=output_hidden_states,\n                return_dict=return_dict,\n            )\n        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n            encoder_outputs = BaseModelOutput(\n                last_hidden_state=encoder_outputs[0],\n                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n            )\n\n        encoder_attention_mask = attention_mask\n        # input modality = speech so new attention mask\n        if self.current_modality == \"speech\" and attention_mask is not None:\n            sub_sampled_lengths = self._compute_sub_sample_lengths_from_attention_mask(attention_mask).to(\n                encoder_outputs[0].device\n            )\n            encoder_attention_mask = _compute_new_attention_mask(\n                hidden_states=encoder_outputs[0], seq_lens=sub_sampled_lengths\n            )\n\n        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n        decoder_outputs = self.text_decoder(\n            input_ids=decoder_input_ids,\n            attention_mask=decoder_attention_mask,\n            encoder_hidden_states=encoder_outputs[0],\n            encoder_attention_mask=encoder_attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=decoder_inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        lm_logits = self.lm_head(decoder_outputs[0])\n\n        masked_lm_loss = None\n        if labels is not None:\n            loss_fct = CrossEntropyLoss()\n            labels = labels.to(lm_logits.device)\n            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n\n        if not return_dict:\n            outputs = decoder_outputs + encoder_outputs\n            output = (lm_logits,) + outputs[1:]\n            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n\n        return Seq2SeqLMOutput(\n            loss=masked_lm_loss,\n            logits=lm_logits,\n            past_key_values=decoder_outputs.past_key_values,\n            decoder_hidden_states=decoder_outputs.hidden_states,\n            decoder_attentions=decoder_outputs.attentions,\n            cross_attentions=decoder_outputs.cross_attentions,\n            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n            encoder_hidden_states=encoder_outputs.hidden_states,\n            encoder_attentions=encoder_outputs.attentions,\n        )\n\n    @torch.no_grad()\n    def generate(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        input_features: Optional[torch.Tensor] = None,\n        return_intermediate_token_ids: Optional[bool] = None,\n        tgt_lang: Optional[str] = None,\n        spkr_id: Optional[int] = 0,\n        generate_speech: Optional[bool] = True,\n        **kwargs,\n    ) -> Union[torch.Tensor, SeamlessM4TGenerationOutput]:\n        \"\"\"\n        Generates translated token ids and/or translated audio waveforms.\n\n        <Tip>\n\n        This method successively calls the `.generate` function of two different sub-models. You can specify keyword\n        arguments at two different levels: general arguments that will be passed to both models, or prefixed arguments\n        that will be passed to one of them.\n\n        For example, calling `.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)` will successively\n        perform beam-search decoding on the text model, and multinomial beam-search sampling on the speech model.\n\n        For an overview of generation strategies and code examples, check out the [following\n        guide](./generation_strategies).\n\n        </Tip>\n\n\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Indices of input sequence tokens in the vocabulary.\n\n                Indices can be obtained using [`SeamlessM4TTokenizer`] or [`SeamlessM4TProcessor`]. See\n                [`PreTrainedTokenizer.encode`] and [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*):\n                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n                [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n            return_intermediate_token_ids (`bool`, *optional*):\n                If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want\n                to get translated text alongside the audio. Note that if `generate_speech=True`, this parameter will be\n                ignored.\n            tgt_lang (`str`, *optional*):\n                The language to use as target language for translation.\n            spkr_id (`int`, *optional*, defaults to 0):\n                The id of the speaker used for speech synthesis. Must be lower than `config.vocoder_num_spkrs`.\n            generate_speech (`bool`, *optional*, defaults to `True`):\n                If `False`, will only returns the text tokens and won't generate speech.\n\n            kwargs (*optional*):\n                Remaining dictionary of keyword arguments that will be passed to [`GenerationMixin.generate`]. Keyword\n                arguments are of two types:\n\n                    - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model,\n                    except for `decoder_input_ids` which will only be passed through the text components.\n                    - With a *text_* or *speech_* prefix, they will be input for the `generate` method of the\n                    text model and speech model respectively. It has the priority over the keywords without a prefix.\n\n                    This means you can, for example, specify a generation strategy for one generation but not for the\n                    other.\n\n        Returns:\n            `Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]`:\n            - If `generate_speech` and `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].\n            - If `generate_speech` and not `return_intermediate_token_ids`, returns a tuple composed of waveforms of\n              shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives the length of each sample.\n            - If `generate_speech=False`, it will returns `ModelOutput`.\n        \"\"\"\n        if input_ids is None and input_features is None and kwargs.get(\"inputs_embeds\", None) is None:\n            raise ValueError(\n                \"`input_ids`,`input_features` and `inputs_embeds` are all empty. Make sure at least one of them is not.\"\n            )\n\n        if generate_speech and tgt_lang is None:\n            raise ValueError(\"You must specify a `tgt_lang` to generate translated speech.\")\n\n        if tgt_lang is not None:\n            # also accept __xxx__\n            tgt_lang = tgt_lang.replace(\"__\", \"\")\n            for key in [\"text_decoder_lang_to_code_id\", \"t2u_lang_code_to_id\", \"vocoder_lang_code_to_id\"]:\n                lang_code_to_id = getattr(self.generation_config, key, None)\n                if lang_code_to_id is None:\n                    raise ValueError(\n                        f\"\"\"This model generation config doesn't have a `{key}` key which maps the target language\n                        to the right token id. Make sure to load the right generation config.\"\"\"\n                    )\n                elif tgt_lang not in lang_code_to_id:\n                    raise ValueError(\n                        f\"\"\"`tgt_lang={tgt_lang}` is not supported by this model.\n                    Please specify a `tgt_lang` in {','.join(lang_code_to_id.keys())}. Note that SeamlessM4T supports\n                    more languages for text translation than for speech synthesis.\"\"\"\n                    )\n\n        batch_size = (\n            len(input_features)\n            if input_features is not None\n            else (len(input_ids) if input_ids is not None else len(kwargs.get(\"inputs_embeds\")))\n        )\n\n        kwargs_text, kwargs_speech = format_speech_generation_kwargs(kwargs)\n        kwargs_text[\"output_hidden_states\"] = True\n        kwargs_text[\"return_dict_in_generate\"] = True\n        kwargs_text[\"output_scores\"] = True\n\n        text_decoder_input_ids = kwargs_text.get(\"decoder_input_ids\")\n        # overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.\n        if tgt_lang is not None:\n            # tgt_lang gets priority over decoder input ids\n            text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n            text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n\n        kwargs_text[\"decoder_input_ids\"] = text_decoder_input_ids\n\n        # first generation\n        if input_features is not None:\n            self.set_modality(\"speech\")\n            if input_ids is not None:\n                logger.warning(\n                    \"`input_features` and `input_ids` are both non empty. `input_features` will be used in priority \"\n                    \"through the speech encoder. Make sure `input_features=None` if you want to use the text encoder.\"\n                )\n            text_generation_output = super().generate(input_features=input_features, **kwargs_text)\n        else:\n            self.set_modality(\"text\")\n            text_generation_output = super().generate(input_ids=input_ids, input_features=None, **kwargs_text)\n        sequences = text_generation_output.sequences\n\n        if not generate_speech:\n            return text_generation_output\n\n        # prepare second generation\n        num_return_sequences = len(sequences) // batch_size\n        attention_mask = kwargs_speech.get(\"attention_mask\", kwargs_text.get(\"attention_mask\", None))\n\n        # get encoder last hidden states\n        if self.current_modality == \"speech\":\n            # get last_hidden_state from encoder - must do a pass through the speech encoder\n            encoder_hidden_states = self.speech_encoder(\n                input_features=input_features, attention_mask=attention_mask\n            ).last_hidden_state\n\n            # input modality = speech so new attention mask for the decoder\n            if attention_mask is not None:\n                sub_sampled_lengths = self._compute_sub_sample_lengths_from_attention_mask(attention_mask).to(\n                    encoder_hidden_states.device\n                )\n                attention_mask = _compute_new_attention_mask(\n                    hidden_states=encoder_hidden_states, seq_lens=sub_sampled_lengths\n                )\n        else:\n            encoder_hidden_states = text_generation_output.encoder_hidden_states[-1]\n\n        # take care of num_return_sequences\n        # take most probable hidden states per batch of return_sequences\n        # (batch_size*num_return_sequences, ...) -> (batch_size,...)\n        if num_return_sequences > 1:\n            idx_most_probable_sequences_per_batch = text_generation_output.sequences_scores.view(batch_size, -1)\n            idx_most_probable_sequences_per_batch = idx_most_probable_sequences_per_batch.argmax(-1)\n            idx_most_probable_sequences_per_batch = (\n                idx_most_probable_sequences_per_batch + torch.arange(batch_size).to(self.device) * num_return_sequences\n            )\n            sequences = sequences[idx_most_probable_sequences_per_batch]\n\n        # get decoder last hidden state - must do a pass through the text decoder\n        t2u_input_embeds = self.text_decoder(\n            input_ids=sequences,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=attention_mask,\n        ).last_hidden_state\n\n        pad_token_id = self.generation_config.pad_token_id\n\n        # Compute new attention mask\n        seq_lens = (sequences != pad_token_id).int().sum(1)\n        t2u_model_attention_mask = _compute_new_attention_mask(t2u_input_embeds, seq_lens)\n        kwargs_speech[\"attention_mask\"] = t2u_model_attention_mask\n\n        # Compute t2u decoder_input_ids\n        t2u_decoder_input_ids = kwargs_speech.get(\"decoder_input_ids\")\n        t2u_tgt_lang_id = self.generation_config.t2u_lang_code_to_id.get(tgt_lang)\n        t2u_decoder_input_ids = torch.tensor([[self.config.t2u_eos_token_id, t2u_tgt_lang_id]] * batch_size).to(\n            self.device\n        )\n        kwargs_speech[\"decoder_input_ids\"] = t2u_decoder_input_ids\n\n        # second generation\n        unit_ids = self.t2u_model.generate(inputs_embeds=t2u_input_embeds, **kwargs_speech)\n        output_unit_ids = unit_ids.detach().clone()\n\n        # get rid of t2u_decoder_input_ids\n        unit_ids = unit_ids[:, kwargs_speech[\"decoder_input_ids\"].shape[1] :]\n        # replace eos per pad\n        unit_ids[unit_ids == self.config.t2u_eos_token_id] = self.config.t2u_pad_token_id\n        # offset of control symbols\n        unit_ids = torch.where(\n            unit_ids == self.config.t2u_pad_token_id, unit_ids, unit_ids - self.config.vocoder_offset\n        )\n\n        vocoder_tgt_lang_id = self.generation_config.vocoder_lang_code_to_id.get(tgt_lang)\n        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids)).to(self.device)\n\n        spkr_id = torch.tensor([[spkr_id]] * len(unit_ids)).to(self.device)\n\n        waveform, waveform_lengths = self.vocoder(input_ids=unit_ids, spkr_id=spkr_id, lang_id=vocoder_tgt_lang_id)\n\n        if return_intermediate_token_ids:\n            return SeamlessM4TGenerationOutput(\n                waveform=waveform,\n                waveform_lengths=waveform_lengths,\n                sequences=sequences,\n                unit_sequences=output_unit_ids,\n            )\n\n        return waveform, waveform_lengths\n\n    def prepare_inputs_for_generation(\n        self,\n        decoder_input_ids,\n        past_key_values=None,\n        attention_mask=None,\n        use_cache=None,\n        encoder_outputs=None,\n        **kwargs,\n    ):\n        # cut decoder_input_ids if past is used\n        if past_key_values is not None:\n            decoder_input_ids = decoder_input_ids[:, -1:]\n\n        return {\n            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n            \"encoder_outputs\": encoder_outputs,\n            \"past_key_values\": past_key_values,\n            \"decoder_input_ids\": decoder_input_ids,\n            \"attention_mask\": attention_mask,\n            \"use_cache\": use_cache,\n        }\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            # cached cross_attention states don't have to be reordered -> they are always the same\n            reordered_past += (\n                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n            )\n        return reordered_past\n\n```\n\n CODE FILE PATH#2: src/transformers/models/seamless_m4t_v2/configuration_seamless_m4t_v2.py\n\n CODE EXPLANATION: This Python script defines the configuration for the SeamlessM4Tv2 model, which is a part of the Hugging Face Transformers library. The configuration is defined in the `SeamlessM4Tv2Config` class, which inherits from the `PretrainedConfig` class. \n\nThe `SeamlessM4Tv2Config` class is initialized with a large number of parameters, each of which corresponds to a specific aspect of the model's architecture or behavior. These parameters include the size of the vocabulary, the number of hidden layers, the number of attention heads, the dropout rate, and many others. \n\nThe parameters are grouped into several categories, each corresponding to a specific part of the model:\n\n1. **Shared Config**: These parameters are common to all parts of the model, such as the size of the hidden layers, the dropout rate, and the activation function.\n\n2. **Text Encoder and Decoder**: These parameters are specific to the text encoder and decoder parts of the model, such as the number of layers and the dimension of the feed-forward layer.\n\n3. **Speech Encoder**: These parameters are specific to the speech encoder part of the model, such as the number of layers and the dropout probability.\n\n4. **Text-To-Unit (t2u) Model**: These parameters are specific to the text-to-unit part of the model, such as the number of encoder layers and the dimension of the feed-forward layer.\n\n5. **Hifi-Gan Vocoder**: These parameters are specific to the Hifi-Gan vocoder part of the model, such as the sampling rate and the number of initial channels.\n\nThe `SeamlessM4Tv2Config` class also includes a `model_type` attribute, which is set to \"seamless_m4t_v2\". This attribute is used by the Hugging Face Transformers library to identify the type of the model.\n\nIn addition to the `SeamlessM4Tv2Config` class, the script also defines a `logger` object for logging, and a `SEAMLESS_M4T_V2_PRETRAINED_CONFIG_ARCHIVE_MAP` dictionary, which maps model identifiers to URLs where their pretrained configurations can be downloaded.\n\nIn summary, this script provides the configuration for the SeamlessM4Tv2 model, allowing users to customize the model's architecture and behavior by adjusting the configuration parameters.\n\n CODE:\n```\n# coding=utf-8\n# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" SeamlessM4Tv2 model configuration\"\"\"\n\nfrom ...configuration_utils import PretrainedConfig\nfrom ...utils import logging\n\n\nlogger = logging.get_logger(__name__)\n\nSEAMLESS_M4T_V2_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n    \"\": \"https://huggingface.co//resolve/main/config.json\",\n}\n\n\nclass SeamlessM4Tv2Config(PretrainedConfig):\n    r\"\"\"\n    This is the configuration class to store the configuration of a [`~SeamlessM4Tv2Model`]. It is used to instantiate\n    an SeamlessM4Tv2 model according to the specified arguments, defining the model architecture. Instantiating a\n    configuration with the defaults will yield a similar configuration to that of the SeamlessM4Tv2\n    [\"\"](https://huggingface.co/\"\") architecture.\n\n    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n    documentation from [`PretrainedConfig`] for more information.\n\n\n    Args:\n        vocab_size (`int`, *optional*, defaults to 256102):\n            Vocabulary size of the text modality of the SeamlessM4Tv2 model. Defines the number of different tokens\n            that can be represented by the `inputs_ids` passed when calling [`~SeamlessM4Tv2Model`],\n            [`~SeamlessM4Tv2ForTextToSpeech`] or [`~SeamlessM4Tv2ForTextToText`].\n        t2u_vocab_size (`int`, *optional*, defaults to 10082):\n            Unit vocabulary size of the SeamlessM4Tv2 model. Defines the number of different \"unit tokens\" that can be\n            represented by the `inputs_ids` passed when calling the Text-To-Units sub-model of [`~SeamlessM4Tv2Model`],\n            [`~SeamlessM4Tv2ForSpeechToSpeech`] or [`~SeamlessM4Tv2ForTextToSpeech`].\n        char_vocab_size (`int`, *optional*, defaults to 10943):\n            Character vocabulary size of the SeamlessM4Tv2 model. Defines the number of different character tokens that\n            can be represented by the `char_inputs_ids` passed when calling the Text-To-Units sub-model of\n            [`~SeamlessM4Tv2Model`], [`~SeamlessM4Tv2ForSpeechToSpeech`] or [`~SeamlessM4Tv2ForTextToSpeech`].\n\n        > Parameters shared across sub-models\n\n        hidden_size (`int`, *optional*, defaults to 1024):\n            Dimensionality of the \"intermediate\" layers in the architecture.\n        initializer_range (`float`, *optional*, defaults to 0.02):\n            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n            The epsilon used by the layer normalization layers.\n        use_cache (`bool`, *optional*, defaults to `True`):\n            Whether or not the model should return the last key/values attentions (not used by all models).\n        max_position_embeddings (`int`, *optional*, defaults to 4096):\n            The maximum sequence length that this model text encoder and decoder might ever be used with. Typically set\n            this to something large just in case (e.g., 512 or 1024 or 2048).\n        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n            Whether the model is used as an encoder/decoder or not.\n        encoder_layerdrop (`float`, *optional*, defaults to 0.05):\n            The LayerDrop probability for the encoders. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)\n            for more details.\n        decoder_layerdrop (`float`, *optional*, defaults to 0.05):\n            The LayerDrop probability for the decoders. See the [LayerDrop paper](see https://arxiv.org/abs/1909.11556)\n            for more details.\n        activation_function (`str` or `function`, *optional*, defaults to `\"relu\"`):\n            The non-linear activation function (function or string) in the decoder and feed-forward layers. If string,\n            `\"gelu\"`, `\"relu\"`, `\"selu\"`, `\"swish\"` and `\"gelu_new\"` are supported.\n        dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all fully connected layers in the embeddings, encoder, decoder, and pooler.\n        attention_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all attention layers.\n        activation_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout probability for all activation layers in the model.\n        scale_embedding (`bool`, *optional*, defaults to `True`):\n            Scale embeddings by diving by sqrt(d_model).\n\n        > Text encoder and text decoder specific parameters\n\n        encoder_layers (`int`, *optional*, defaults to 24):\n            Number of hidden layers in the Transformer text encoder.\n        encoder_ffn_dim (`int`, *optional*, defaults to 8192):\n            Dimension of the \"intermediate\" (i.e., feed-forward) layer in the Transformer text encoder.\n        encoder_attention_heads (`int`, *optional*, defaults to 16):\n            Number of attention heads for each attention layer in the Transformer text encoder.\n        decoder_layers (`int`, *optional*, defaults to 24):\n            Number of hidden layers in the Transformer text decoder.\n        decoder_ffn_dim (`int`, *optional*, defaults to 8192):\n            Dimension of the \"intermediate\" (i.e., feed-forward) layer in the Transformer text decoder.\n        decoder_attention_heads (`int`, *optional*, defaults to 16):\n            Number of attention heads for each attention layer in the Transformer text decoder.\n        decoder_start_token_id (`int`, *optional*, defaults to 3):\n            If an encoder-decoder model starts decoding with a different token than _bos_, the id of that token. Only\n            applied in the text decoder.\n        max_new_tokens (`int`, *optional*, defaults to 256):\n            The maximum numbers of text tokens to generate, ignoring the number of tokens in the prompt.\n        pad_token_id (`int`, *optional*, defaults to 0):\n            The id of the _padding_ text token. Only applied to the text-decoder model.\n        bos_token_id (`int`, *optional*, defaults to 2):\n            The id of the _beginning-of-stream_ text token. Only applied to the text-decoder model.\n        eos_token_id (`int`, *optional*, defaults to 3):\n            The id of the _end-of-stream_ text token. Only applied to the text-decoder model.\n\n        > Speech encoder specific parameters\n\n        speech_encoder_layers (`int`, *optional*, defaults to 24):\n            Number of hidden layers in the Transformer speech encoder.\n        speech_encoder_attention_heads (`int`, *optional*, defaults to 16):\n            Number of attention heads for each attention layer in the Transformer speech encoder.\n        speech_encoder_intermediate_size (`int`, *optional*, defaults to 4096):\n            Dimension of the \"intermediate\" (i.e., feed-forward) layer in the Transformer speech encoder.\n        speech_encoder_hidden_act (`str` or `function`, *optional*, defaults to `\"swish\"`):\n            The non-linear activation function (function or string) in the speech encoder. If string, `\"gelu\"`,\n            `\"relu\"`, `\"selu\"`, `\"swish\"` and `\"gelu_new\"` are supported.\n        speech_encoder_dropout (`float`, *optional*, defaults to 0.0):\n            The dropout probability for all layers in the speech encoder.\n        add_adapter (`bool`, *optional*, defaults to `True`):\n            Add an adapter layer on top of the speech encoder.\n        speech_encoder_layerdrop (`float`, *optional*, defaults to 0.1):\n            The LayerDrop probability for the speech encoder. See the [LayerDrop paper](see\n            https://arxiv.org/abs/1909.11556) for more details.\n        feature_projection_input_dim (`int`, *optional*, defaults to 160):\n            Input dimension of the input feature projection of the speech encoder, i.e the dimension after processing\n            input audios with [`SeamlessM4TFeatureExtractor`].\n        adaptor_kernel_size (`int`, *optional*, defaults to 8):\n            Kernel size of the convolutional layers in the adapter network. Only relevant if `add_adapter is True`.\n        adaptor_stride (`int`, *optional*, defaults to 8):\n            Stride of the convolutional layers in the adapter network. Only relevant if `add_adapter is True`.\n        adaptor_dropout (`float`, *optional*, defaults to 0.1):\n            The dropout probability for all layers in the speech adapter.\n        num_adapter_layers (`int`, *optional*, defaults to 1):\n            Number of convolutional layers that should be used in the adapter network. Only relevant if `add_adapter is\n            True`.\n        position_embeddings_type (`str`, *optional*, defaults to `\"relative_key\"`):\n            Can be specified to `relative_key`. If left to `None`, no relative position embedding is applied. Only\n            applied to the speech encoder. For more information on `\"relative_key\"`, please refer to [Self-Attention\n            with Relative Position Representations (Shaw et al.)](https://arxiv.org/abs/1803.02155).\n        conv_depthwise_kernel_size (`int`, *optional*, defaults to 31):\n            Kernel size of convolutional depthwise 1D layer in Conformer blocks. Only applied to the speech encoder.\n        left_max_position_embeddings (`int`, *optional*, defaults to 64):\n            The left clipping value for relative positions.\n        right_max_position_embeddings (`int`, *optional*, defaults to 8):\n            The right clipping value for relative positions.\n        speech_encoder_chunk_size (`int`, *optional*, defaults to 20000): The size of each attention chunk.\n        speech_encoder_left_chunk_num (`int`, *optional*, defaults to 128):\n            Number of chunks on the left up to which lookahead is allowed.\n\n        > Text-To-Unit (t2u) model specific parameters\n\n        t2u_bos_token_id (`int`, *optional*, defaults to 0):\n            The id of the _beginning-of-stream_ unit token. Only applied to the text-to-unit seq2seq model.\n        t2u_pad_token_id (`int`, *optional*, defaults to 1):\n            The id of the _padding_ unit token. Only applied to the text-to-unit seq2seq model.\n        t2u_eos_token_id (`int`, *optional*, defaults to 2):\n            The id of the _end-of-stream_ unit token. Only applied to the text-to-unit seq2seq model.\n        t2u_encoder_layers (`int`, *optional*, defaults to 6):\n            Number of hidden layers in the Transformer text-to-unit encoder.\n        t2u_encoder_ffn_dim (`int`, *optional*, defaults to 8192):\n            Dimension of the \"intermediate\" (i.e., feed-forward) layer in the Transformer text-to-unit encoder.\n        t2u_encoder_attention_heads (`int`, *optional*, defaults to 16):\n            Number of attention heads for each attention layer in the Transformer text-to-unit encoder.\n        t2u_decoder_layers (`int`, *optional*, defaults to 6):\n            Number of hidden layers in the Transformer text-to-unit decoder.\n        t2u_decoder_ffn_dim (`int`, *optional*, defaults to 8192):\n            Dimension of the \"intermediate\" (i.e., feed-forward) layer in the Transformer text-to-unit decoder.\n        t2u_decoder_attention_heads (`int`, *optional*, defaults to 16):\n            Number of attention heads for each attention layer in the Transformer text-to-unit decoder.\n        t2u_max_position_embeddings (`int`, *optional*, defaults to 4096):\n            The maximum sequence length that this model text-to-unit component might ever be used with. Typically set\n            this to something large just in case (e.g., 512 or 1024 or 2048).\n        t2u_variance_predictor_embed_dim (`int`, *optional*, defaults to 1024):\n            The projection dimension of the text-to-unit's duration predictor.\n        t2u_variance_predictor_hidden_dim (`int`, *optional*, defaults to 256):\n            Internal dimension of the text-to-unit's duration predictor.\n        t2u_variance_predictor_kernel_size (`int`, *optional*, defaults to 3):\n            Kernel size of the convolutional layers of the text-to-unit's duration predictor.\n        t2u_variance_pred_dropout (`float`, *optional*, defaults to 0.5):\n            The dropout probability of the text-to-unit's duration predictor.\n\n         > Hifi-Gan Vocoder specific parameters\n\n        sampling_rate (`int`, *optional*, defaults to 16000):\n            The sampling rate at which the output audio will be generated, expressed in hertz (Hz).\n        upsample_initial_channel (`int`, *optional*, defaults to 512):\n            The number of input channels into the hifi-gan upsampling network. Applies to the vocoder only.\n        upsample_rates (`Tuple[int]` or `List[int]`, *optional*, defaults to `[5, 4, 4, 2, 2]`):\n            A tuple of integers defining the stride of each 1D convolutional layer in the vocoder upsampling network.\n            The length of *upsample_rates* defines the number of convolutional layers and has to match the length of\n            *upsample_kernel_sizes*. Applies to the vocoder only.\n        upsample_kernel_sizes (`Tuple[int]` or `List[int]`, *optional*, defaults to `[11, 8, 8, 4, 4]`):\n            A tuple of integers defining the kernel size of each 1D convolutional layer in the vocoder upsampling\n            network. The length of *upsample_kernel_sizes* defines the number of convolutional layers and has to match\n            the length of *upsample_rates*. Applies to the vocoder only.\n        resblock_kernel_sizes (`Tuple[int]` or `List[int]`, *optional*, defaults to `[3, 7, 11]`):\n            A tuple of integers defining the kernel sizes of the vocoder 1D convolutional layers in the multi-receptive\n            field fusion (MRF) module. Applies to the vocoder only.\n        resblock_dilation_sizes (`Tuple[Tuple[int]]` or `List[List[int]]`, *optional*, defaults to `[[1, 3, 5], [1, 3, 5], [1, 3, 5]]`):\n            A nested tuple of integers defining the dilation rates of the vocoder dilated 1D convolutional layers in\n            the multi-receptive field fusion (MRF) module. Applies to the vocoder only.\n        leaky_relu_slope (`float`, *optional*, defaults to 0.1):\n            The angle of the negative slope used by the leaky ReLU activation in the vocoder. Applies to the vocoder\n            only.\n        unit_hifi_gan_vocab_size (`int`, *optional*, defaults to 10000):\n            Vocabulary size of the SeamlessM4Tv2 vocoder. Defines the number of different unit tokens that can be\n            represented by the `inputs_ids` passed when calling the vocoder of [`~SeamlessM4Tv2Model`],\n            [`~SeamlessM4Tv2ForSpeechToSpeech`] or [`~SeamlessM4Tv2ForTextToSpeech`].\n        unit_embed_dim (`int`, *optional*, defaults to 1280):\n            The projection dimension of the input ids given to the hifi-gan vocoder. Applies to the vocoder only.\n        lang_embed_dim (`int`, *optional*, defaults to 256):\n            The projection dimension of the target language given to the hifi-gan vocoder. Applies to the vocoder only.\n        spkr_embed_dim (`int`, *optional*, defaults to 256):\n            The projection dimension of the speaker id given to the hifi-gan vocoder. Applies to the vocoder only.\n        vocoder_num_langs (`int`, *optional*, defaults to 36):\n            Number of langs supported by the vocoder. Might be different from `t2u_num_langs`.\n        vocoder_num_spkrs (`int`, *optional*, defaults to 200):\n            Number of speakers supported by the vocoder.\n        variance_predictor_kernel_size (`int`, *optional*, defaults to 3):\n            Kernel size of the duration predictor. Applies to the vocoder only.\n        var_pred_dropout (`float`, *optional*, defaults to 0.5):\n            The dropout probability of the duration predictor. Applies to the vocoder only.\n        vocoder_offset (`int`, *optional*, defaults to 4):\n            Offset the unit token ids by this number to account for symbol tokens. Applies to the vocoder only.\n\n    ```python\n    >>> from transformers import SeamlessM4Tv2Model, SeamlessM4Tv2Config\n\n    >>> # Initializing a SeamlessM4Tv2 \"\" style configuration\n    >>> configuration = SeamlessM4Tv2Config()\n\n    >>> # Initializing a model from the \"\" style configuration\n    >>> model = SeamlessM4Tv2Model(configuration)\n\n    >>> # Accessing the model configuration\n    >>> configuration = model.config\n    ```\"\"\"\n\n    model_type = \"seamless_m4t_v2\"\n\n    def __init__(\n        self,\n        vocab_size=256102,\n        t2u_vocab_size=10082,\n        char_vocab_size=10943,\n        # shared config\n        hidden_size=1024,\n        initializer_range=0.02,\n        layer_norm_eps=1e-5,\n        use_cache=True,\n        max_position_embeddings=4096,\n        is_encoder_decoder=True,\n        encoder_layerdrop=0.05,\n        decoder_layerdrop=0.05,\n        activation_function=\"relu\",\n        dropout=0.1,\n        attention_dropout=0.1,\n        activation_dropout=0.0,\n        scale_embedding=True,\n        # text encoder|decoder\n        encoder_layers=24,\n        encoder_ffn_dim=8192,\n        encoder_attention_heads=16,\n        decoder_layers=24,\n        decoder_ffn_dim=8192,\n        decoder_attention_heads=16,\n        decoder_start_token_id=3,\n        max_new_tokens=256,\n        pad_token_id=0,\n        bos_token_id=2,\n        eos_token_id=3,\n        # speech_encoder\n        speech_encoder_layers=24,\n        speech_encoder_attention_heads=16,\n        speech_encoder_intermediate_size=4096,\n        speech_encoder_hidden_act=\"swish\",\n        speech_encoder_dropout=0.0,\n        add_adapter=True,\n        speech_encoder_layerdrop=0.1,\n        feature_projection_input_dim=160,\n        adaptor_kernel_size=8,\n        adaptor_stride=8,\n        adaptor_dropout=0.1,\n        num_adapter_layers=1,\n        position_embeddings_type=\"relative_key\",\n        conv_depthwise_kernel_size=31,\n        left_max_position_embeddings=64,\n        right_max_position_embeddings=8,\n        speech_encoder_chunk_size=20000,\n        speech_encoder_left_chunk_num=128,\n        # t2u config\n        t2u_bos_token_id=0,\n        t2u_pad_token_id=1,\n        t2u_eos_token_id=2,\n        t2u_encoder_layers=6,\n        t2u_encoder_ffn_dim=8192,\n        t2u_encoder_attention_heads=16,\n        t2u_decoder_layers=6,\n        t2u_decoder_ffn_dim=8192,\n        t2u_decoder_attention_heads=16,\n        t2u_max_position_embeddings=4096,\n        t2u_variance_predictor_embed_dim=1024,\n        t2u_variance_predictor_hidden_dim=256,\n        t2u_variance_predictor_kernel_size=3,\n        t2u_variance_pred_dropout=0.5,\n        # hifi-gan vocoder config\n        sampling_rate=16000,\n        upsample_initial_channel=512,\n        upsample_rates=[5, 4, 4, 2, 2],\n        upsample_kernel_sizes=[11, 8, 8, 4, 4],\n        resblock_kernel_sizes=[3, 7, 11],\n        resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n        leaky_relu_slope=0.1,\n        # specific to Code Hifi-Gan\n        unit_hifi_gan_vocab_size=10000,\n        unit_embed_dim=1280,\n        lang_embed_dim=256,\n        spkr_embed_dim=256,\n        vocoder_num_langs=36,\n        vocoder_num_spkrs=200,\n        variance_predictor_kernel_size=3,\n        var_pred_dropout=0.5,\n        vocoder_offset=4,\n        **kwargs,\n    ):\n        # overall_config\n        self.vocab_size = vocab_size\n        self.t2u_vocab_size = t2u_vocab_size\n        self.char_vocab_size = char_vocab_size\n        self.hidden_size = hidden_size\n        self.initializer_range = initializer_range\n        self.layer_norm_eps = layer_norm_eps\n        self.max_position_embeddings = max_position_embeddings\n        self.use_cache = use_cache\n        self.max_new_tokens = max_new_tokens\n        self.encoder_layerdrop = encoder_layerdrop\n        self.decoder_layerdrop = decoder_layerdrop\n        self.activation_function = activation_function\n        self.dropout = dropout\n        self.attention_dropout = attention_dropout\n        self.activation_dropout = activation_dropout\n        self.scale_embedding = scale_embedding\n        # for proper config init\n        self.num_attention_heads = decoder_attention_heads\n        self.num_hidden_layers = decoder_layers\n\n        # text|unit encoder|decoder\n        self.encoder_layers = encoder_layers\n        self.encoder_ffn_dim = encoder_ffn_dim\n        self.encoder_attention_heads = encoder_attention_heads\n        self.decoder_layers = decoder_layers\n        self.decoder_ffn_dim = decoder_ffn_dim\n        self.decoder_attention_heads = decoder_attention_heads\n\n        # speech_encoder\n        self.speech_encoder_layers = speech_encoder_layers\n        self.speech_encoder_hidden_act = speech_encoder_hidden_act\n        self.speech_encoder_dropout = speech_encoder_dropout\n        self.speech_encoder_attention_heads = speech_encoder_attention_heads\n        self.speech_encoder_layerdrop = speech_encoder_layerdrop\n        self.speech_encoder_intermediate_size = speech_encoder_intermediate_size\n        self.feature_projection_input_dim = feature_projection_input_dim\n        self.adaptor_kernel_size = adaptor_kernel_size\n        self.adaptor_stride = adaptor_stride\n        self.adaptor_dropout = adaptor_dropout\n        self.num_adapter_layers = num_adapter_layers\n        self.position_embeddings_type = position_embeddings_type\n        self.conv_depthwise_kernel_size = conv_depthwise_kernel_size\n        self.add_adapter = add_adapter\n        self.left_max_position_embeddings = left_max_position_embeddings\n        self.right_max_position_embeddings = right_max_position_embeddings\n        self.speech_encoder_chunk_size = speech_encoder_chunk_size\n        self.speech_encoder_left_chunk_num = speech_encoder_left_chunk_num\n\n        # t2u config\n        self.t2u_bos_token_id = t2u_bos_token_id\n        self.t2u_pad_token_id = t2u_pad_token_id\n        self.t2u_eos_token_id = t2u_eos_token_id\n        self.t2u_encoder_layers = t2u_encoder_layers\n        self.t2u_encoder_ffn_dim = t2u_encoder_ffn_dim\n        self.t2u_encoder_attention_heads = t2u_encoder_attention_heads\n        self.t2u_decoder_layers = t2u_decoder_layers\n        self.t2u_decoder_ffn_dim = t2u_decoder_ffn_dim\n        self.t2u_decoder_attention_heads = t2u_decoder_attention_heads\n        self.t2u_max_position_embeddings = t2u_max_position_embeddings\n        self.t2u_variance_predictor_embed_dim = t2u_variance_predictor_embed_dim  # TODO: add to docstrings\n        self.t2u_variance_predictor_hidden_dim = t2u_variance_predictor_hidden_dim  # TODO: add to docstrings\n        self.t2u_variance_predictor_kernel_size = t2u_variance_predictor_kernel_size  # TODO: add to docstrings\n        self.t2u_variance_pred_dropout = t2u_variance_pred_dropout  # TODO: add to docstrings\n\n        # hifi-gan vocoder config\n        # original parameters specific to Hifi-Gan\n        self.sampling_rate = sampling_rate\n        self.upsample_initial_channel = upsample_initial_channel\n        self.upsample_rates = upsample_rates\n        self.upsample_kernel_sizes = upsample_kernel_sizes\n        self.resblock_kernel_sizes = resblock_kernel_sizes\n        self.resblock_dilation_sizes = resblock_dilation_sizes\n        self.leaky_relu_slope = leaky_relu_slope\n\n        # specific to Code Hifi-Gan\n        self.unit_hifi_gan_vocab_size = unit_hifi_gan_vocab_size\n        self.unit_embed_dim = unit_embed_dim\n        self.lang_embed_dim = lang_embed_dim\n        self.spkr_embed_dim = spkr_embed_dim\n        self.vocoder_num_langs = vocoder_num_langs\n        self.vocoder_num_spkrs = vocoder_num_spkrs\n        self.variance_predictor_kernel_size = variance_predictor_kernel_size\n        self.var_pred_dropout = var_pred_dropout\n        self.vocoder_offset = vocoder_offset\n\n        super().__init__(\n            pad_token_id=pad_token_id,\n            bos_token_id=bos_token_id,\n            eos_token_id=eos_token_id,\n            decoder_start_token_id=decoder_start_token_id,\n            is_encoder_decoder=is_encoder_decoder,\n            max_position_embeddings=max_position_embeddings,\n            **kwargs,\n        )\n\n```\n"}, {"role": "system", "content": "REITERATING THE INSTRUCTIONS: DO NOT REPEAT THE INSTRUCTIONS IN THE RESPONSE.###INSTRUCTION###\nYou are Fluffy, a tech solutions expert and maintainer of the repo https://github.com/huggingface/transformers. Your task is to help developers in addressing a user query related to the opensource github repo.\nUse the context provided below to answer the user query.\n    Guidelines for Fluffy:\n    1. Think sequentially, breaking the user query step by step.\n    2. When uncertain, answer with \"I don't know\".\n    3. When there is any question asked outside the scope of the repository, respond with \u201cI can only assist you in understanding how the repo works better.\u201d\n"}, {"role": "user", "content": "REITERATING MY QUERY:past_key_values for SeamlessM4Tv2ForSpeechToText is not working as expected\nI have segments of audio, and I would like to pass past_key_values between them. I exptected the transcribtion quality to increase, but rather it became unreadable.\n\nprocessor = AutoProcessor.from_pretrained(path_to_model)\nmodel = SeamlessM4Tv2ForSpeechToText.from_pretrained(path_to_model)\naudio_chunks = [audio_segments]\n\npast_key_values= None\n\nfor i in range(5):\n    audio_inputs = processor(audios=audio_chunks[i], return_tensors=\"pt\", sampling_rate=16_000)\n\n    output = model.generate(**audio_inputs, tgt_lang=\"rus\", repetition_penalty=1.1, return_dict_in_generate=True, past_key_values=past_key_values)\n    \n    tmp_result = processor.decode(output[0][0], skip_special_tokens=True)\n    past_key_values = output['past_key_values']\n\nExpected behavior\nThe transcription quality is supposed to increase when I pass past_key_values (or at least have similar transcribation quality as when past_key_values=None).\nThe audio is the same. In other words, I had some audio, applied VAD to segment it into batches, and then fed this segments to the model one by one.\n"}]